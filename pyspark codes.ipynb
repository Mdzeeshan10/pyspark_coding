{"cells":[{"cell_type":"markdown","source":["create use own dataframe with schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f5516d6-8029-4694-bff8-5999341abc90","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data = [(1,'umair'),(2,\"pihu\")]\nschema =['id','name']\ndf = spark.createDataFrame(data,schema)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7815f29c-f4e9-455f-be36-355a3ebf6c64","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"umair"],[2,"pihu"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>umair</td></tr><tr><td>2</td><td>pihu</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\ndata = [{'id':1,'name':\"umair\"},{'id':2,'name':\"pihu\"},{\"id\":3,\"name\":\"priyanka\"}]\ndf = spark.createDataFrame(data)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a745b20-86d8-484c-8695-b2ec99e64020","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"umair"],[2,"pihu"],[3,"priyanka"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>umair</td></tr><tr><td>2</td><td>pihu</td></tr><tr><td>3</td><td>priyanka</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\ndata = [{'id':1,'name':\"umair\"},{'id':2,'name':\"pihu\"},{\"id\":3,'name':\"priyanka\"}]\nschema= StructType([StructField(name ='id',dataType=IntegerType()),StructField(name= 'name',dataType=StringType())])\ndf = spark.createDataFrame(data,schema)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"228227f8-6ae6-4941-be9e-dd3ec386c489","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"umair"],[2,"pihu"],[3,"priyanka"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>umair</td></tr><tr><td>2</td><td>pihu</td></tr><tr><td>3</td><td>priyanka</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["type(\"abc\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf3ac968-f257-476f-b945-4a8e4ca9157a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[4]: str"]}],"execution_count":0},{"cell_type":"code","source":["type(spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c3a36f6-25ad-494c-917d-787c881a97d7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[5]: pyspark.sql.session.SparkSession"]}],"execution_count":0},{"cell_type":"code","source":["help(spark.createDataFrame)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67e4489d-25e1-4afe-a5a6-32c1c5ca23a2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Help on method createDataFrame in module pyspark.sql.session:\n\ncreateDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n    or a :class:`numpy.ndarray`.\n    \n    When ``schema`` is a list of column names, the type of each column\n    will be inferred from ``data``.\n    \n    When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n    from ``data``, which should be an RDD of either :class:`Row`,\n    :class:`namedtuple`, or :class:`dict`.\n    \n    When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n    the real data, or an exception will be thrown at runtime. If the given schema is not\n    :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n    :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n    Each record will also be wrapped into a tuple, which can be converted to row later.\n    \n    If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n    rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n    \n    .. versionadded:: 2.0.0\n    \n    .. versionchanged:: 2.1.0\n       Added verifySchema.\n    \n    Parameters\n    ----------\n    data : :class:`RDD` or iterable\n        an RDD of any kind of SQL data representation (:class:`Row`,\n        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n        column names, default is None.  The data type string format equals to\n        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n        omit the ``struct<>``.\n    samplingRatio : float, optional\n        the sample ratio of rows used for inferring\n    verifySchema : bool, optional\n        verify data types of every row against schema. Enabled by default.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n    \n    Notes\n    -----\n    Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n    \n    Examples\n    --------\n    >>> l = [('Alice', 1)]\n    >>> spark.createDataFrame(l).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> spark.createDataFrame(l, ['name', 'age']).collect()\n    [Row(name='Alice', age=1)]\n    \n    >>> d = [{'name': 'Alice', 'age': 1}]\n    >>> spark.createDataFrame(d).collect()\n    [Row(age=1, name='Alice')]\n    \n    >>> rdd = sc.parallelize(l)\n    >>> spark.createDataFrame(rdd).collect()\n    [Row(_1='Alice', _2=1)]\n    >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n    >>> df.collect()\n    [Row(name='Alice', age=1)]\n    \n    >>> from pyspark.sql import Row\n    >>> Person = Row('name', 'age')\n    >>> person = rdd.map(lambda r: Person(*r))\n    >>> df2 = spark.createDataFrame(person)\n    >>> df2.collect()\n    [Row(name='Alice', age=1)]\n    \n    >>> from pyspark.sql.types import *\n    >>> schema = StructType([\n    ...    StructField(\"name\", StringType(), True),\n    ...    StructField(\"age\", IntegerType(), True)])\n    >>> df3 = spark.createDataFrame(rdd, schema)\n    >>> df3.collect()\n    [Row(name='Alice', age=1)]\n    \n    >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n    [Row(name='Alice', age=1)]\n    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n    [Row(0=1, 1=2)]\n    \n    >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n    [Row(a='Alice', b=1)]\n    >>> rdd = rdd.map(lambda row: row[1])\n    >>> spark.createDataFrame(rdd, \"int\").collect()\n    [Row(value=1)]\n    >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    Py4JJavaError: ...\n\n"]}],"execution_count":0},{"cell_type":"code","source":["help(spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d849962-b89f-4a4e-b230-193ce64bf422","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Help on SparkSession in module pyspark.sql.session object:\n\nclass SparkSession(pyspark.sql.pandas.conversion.SparkConversionMixin)\n |  SparkSession(sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})\n |  \n |  The entry point to programming Spark with the Dataset and DataFrame API.\n |  \n |  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n |  tables, execute SQL over tables, cache tables, and read parquet files.\n |  To create a :class:`SparkSession`, use the following builder pattern:\n |  \n |  .. autoattribute:: builder\n |     :annotation:\n |  \n |  Examples\n |  --------\n |  >>> spark = SparkSession.builder \\\n |  ...     .master(\"local\") \\\n |  ...     .appName(\"Word Count\") \\\n |  ...     .config(\"spark.some.config.option\", \"some-value\") \\\n |  ...     .getOrCreate()\n |  \n |  >>> from datetime import datetime\n |  >>> from pyspark.sql import Row\n |  >>> spark = SparkSession(sc)\n |  >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n |  ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n |  ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n |  >>> df = allTypes.toDF()\n |  >>> df.createOrReplaceTempView(\"allTypes\")\n |  >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n |  ...            'from allTypes where b and i > 0').collect()\n |  [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n |  >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n |  [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n |  \n |  Method resolution order:\n |      SparkSession\n |      pyspark.sql.pandas.conversion.SparkConversionMixin\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __enter__(self) -> 'SparkSession'\n |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n |      \n |      .. versionadded:: 2.0\n |  \n |  __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[traceback]) -> None\n |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n |      \n |      Specifically stop the SparkSession on exit of the with block.\n |      \n |      .. versionadded:: 2.0\n |  \n |  __init__(self, sparkContext: pyspark.context.SparkContext, jsparkSession: Optional[py4j.java_gateway.JavaObject] = None, options: Dict[str, Any] = {})\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  createDataFrame(self, data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame\n |      Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n |      or a :class:`numpy.ndarray`.\n |      \n |      When ``schema`` is a list of column names, the type of each column\n |      will be inferred from ``data``.\n |      \n |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n |      from ``data``, which should be an RDD of either :class:`Row`,\n |      :class:`namedtuple`, or :class:`dict`.\n |      \n |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n |      the real data, or an exception will be thrown at runtime. If the given schema is not\n |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\".\n |      Each record will also be wrapped into a tuple, which can be converted to row later.\n |      \n |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      .. versionchanged:: 2.1.0\n |         Added verifySchema.\n |      \n |      Parameters\n |      ----------\n |      data : :class:`RDD` or iterable\n |          an RDD of any kind of SQL data representation (:class:`Row`,\n |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n |          :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n |          column names, default is None.  The data type string format equals to\n |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n |          omit the ``struct<>``.\n |      samplingRatio : float, optional\n |          the sample ratio of rows used for inferring\n |      verifySchema : bool, optional\n |          verify data types of every row against schema. Enabled by default.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Notes\n |      -----\n |      Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.\n |      \n |      Examples\n |      --------\n |      >>> l = [('Alice', 1)]\n |      >>> spark.createDataFrame(l).collect()\n |      [Row(_1='Alice', _2=1)]\n |      >>> spark.createDataFrame(l, ['name', 'age']).collect()\n |      [Row(name='Alice', age=1)]\n |      \n |      >>> d = [{'name': 'Alice', 'age': 1}]\n |      >>> spark.createDataFrame(d).collect()\n |      [Row(age=1, name='Alice')]\n |      \n |      >>> rdd = sc.parallelize(l)\n |      >>> spark.createDataFrame(rdd).collect()\n |      [Row(_1='Alice', _2=1)]\n |      >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n |      >>> df.collect()\n |      [Row(name='Alice', age=1)]\n |      \n |      >>> from pyspark.sql import Row\n |      >>> Person = Row('name', 'age')\n |      >>> person = rdd.map(lambda r: Person(*r))\n |      >>> df2 = spark.createDataFrame(person)\n |      >>> df2.collect()\n |      [Row(name='Alice', age=1)]\n |      \n |      >>> from pyspark.sql.types import *\n |      >>> schema = StructType([\n |      ...    StructField(\"name\", StringType(), True),\n |      ...    StructField(\"age\", IntegerType(), True)])\n |      >>> df3 = spark.createDataFrame(rdd, schema)\n |      >>> df3.collect()\n |      [Row(name='Alice', age=1)]\n |      \n |      >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n |      [Row(name='Alice', age=1)]\n |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n |      [Row(0=1, 1=2)]\n |      \n |      >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n |      [Row(a='Alice', b=1)]\n |      >>> rdd = rdd.map(lambda row: row[1])\n |      >>> spark.createDataFrame(rdd, \"int\").collect()\n |      [Row(value=1)]\n |      >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n |      Traceback (most recent call last):\n |          ...\n |      Py4JJavaError: ...\n |  \n |  newSession(self) -> 'SparkSession'\n |      Returns a new :class:`SparkSession` as new session, that has separate SQLConf,\n |      registered temporary views and UDFs, but shared :class:`SparkContext` and\n |      table cache.\n |      \n |      .. versionadded:: 2.0\n |  \n |  range(self, start: int, end: Optional[int] = None, step: int = 1, numPartitions: Optional[int] = None) -> pyspark.sql.dataframe.DataFrame\n |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n |      step value ``step``.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      start : int\n |          the start value\n |      end : int, optional\n |          the end value (exclusive)\n |      step : int, optional\n |          the incremental step (default: 1)\n |      numPartitions : int, optional\n |          the number of partitions of the DataFrame\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      >>> spark.range(1, 7, 2).collect()\n |      [Row(id=1), Row(id=3), Row(id=5)]\n |      \n |      If only one argument is specified, it will be used as the end value.\n |      \n |      >>> spark.range(3).collect()\n |      [Row(id=0), Row(id=1), Row(id=2)]\n |  \n |  sql(self, sqlQuery: str, **kwargs: Any) -> pyspark.sql.dataframe.DataFrame\n |      Returns a :class:`DataFrame` representing the result of the given query.\n |      When ``kwargs`` is specified, this method formats the given string by using the Python\n |      standard formatter.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      sqlQuery : str\n |          SQL query string.\n |      kwargs : dict\n |          Other variables that the user wants to set that can be referenced in the query\n |      \n |          .. versionchanged:: 3.3.0\n |             Added optional argument ``kwargs`` to specify the mapping of variables in the query.\n |             This feature is experimental and unstable.\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      Executing a SQL query.\n |      \n |      >>> spark.sql(\"SELECT * FROM range(10) where id > 7\").show()\n |      +---+\n |      | id|\n |      +---+\n |      |  8|\n |      |  9|\n |      +---+\n |      \n |      Executing a SQL query with variables as Python formatter standard.\n |      \n |      >>> spark.sql(\n |      ...     \"SELECT * FROM range(10) WHERE id > {bound1} AND id < {bound2}\", bound1=7, bound2=9\n |      ... ).show()\n |      +---+\n |      | id|\n |      +---+\n |      |  8|\n |      +---+\n |      \n |      >>> mydf = spark.range(10)\n |      >>> spark.sql(\n |      ...     \"SELECT {col} FROM {mydf} WHERE id IN {x}\",\n |      ...     col=mydf.id, mydf=mydf, x=tuple(range(4))).show()\n |      +---+\n |      | id|\n |      +---+\n |      |  0|\n |      |  1|\n |      |  2|\n |      |  3|\n |      +---+\n |      \n |      >>> spark.sql('''\n |      ...   SELECT m1.a, m2.b\n |      ...   FROM {table1} m1 INNER JOIN {table2} m2\n |      ...   ON m1.key = m2.key\n |      ...   ORDER BY m1.a, m2.b''',\n |      ...   table1=spark.createDataFrame([(1, \"a\"), (2, \"b\")], [\"a\", \"key\"]),\n |      ...   table2=spark.createDataFrame([(3, \"a\"), (4, \"b\"), (5, \"b\")], [\"b\", \"key\"])).show()\n |      +---+---+\n |      |  a|  b|\n |      +---+---+\n |      |  1|  3|\n |      |  2|  4|\n |      |  2|  5|\n |      +---+---+\n |      \n |      Also, it is possible to query using class:`Column` from :class:`DataFrame`.\n |      \n |      >>> mydf = spark.createDataFrame([(1, 4), (2, 4), (3, 6)], [\"A\", \"B\"])\n |      >>> spark.sql(\"SELECT {df.A}, {df[B]} FROM {df}\", df=mydf).show()\n |      +---+---+\n |      |  A|  B|\n |      +---+---+\n |      |  1|  4|\n |      |  2|  4|\n |      |  3|  6|\n |      +---+---+\n |  \n |  stop(self) -> None\n |      Stop the underlying :class:`SparkContext`.\n |      \n |      .. versionadded:: 2.0\n |  \n |  table(self, tableName: str) -> pyspark.sql.dataframe.DataFrame\n |      Returns the specified table as a :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      >>> df.createOrReplaceTempView(\"table1\")\n |      >>> df2 = spark.table(\"table1\")\n |      >>> sorted(df.collect()) == sorted(df2.collect())\n |      True\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  getActiveSession() -> Optional[ForwardRef('SparkSession')] from builtins.type\n |      Returns the active :class:`SparkSession` for the current thread, returned by the builder\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      Returns\n |      -------\n |      :class:`SparkSession`\n |          Spark session if an active session exists for the current thread\n |      \n |      Examples\n |      --------\n |      >>> s = SparkSession.getActiveSession()\n |      >>> l = [('Alice', 1)]\n |      >>> rdd = s.sparkContext.parallelize(l)\n |      >>> df = s.createDataFrame(rdd, ['name', 'age'])\n |      >>> df.select(\"age\").collect()\n |      [Row(age=1)]\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  catalog\n |      Interface through which the user may create, drop, alter or query underlying\n |      databases, tables, functions, etc.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`Catalog`\n |  \n |  conf\n |      Runtime configuration interface for Spark.\n |      \n |      This is the interface through which the user can get and set all Spark and Hadoop\n |      configurations that are relevant to Spark SQL. When getting the value of a config,\n |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n |      \n |      Returns\n |      -------\n |      :class:`pyspark.sql.conf.RuntimeConfig`\n |      \n |      .. versionadded:: 2.0\n |  \n |  read\n |      Returns a :class:`DataFrameReader` that can be used to read data\n |      in as a :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`DataFrameReader`\n |  \n |  readStream\n |      Returns a :class:`DataStreamReader` that can be used to read data streams\n |      as a streaming :class:`DataFrame`.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      Returns\n |      -------\n |      :class:`DataStreamReader`\n |  \n |  sparkContext\n |      Returns the underlying :class:`SparkContext`.\n |      \n |      .. versionadded:: 2.0\n |  \n |  streams\n |      Returns a :class:`StreamingQueryManager` that allows managing all the\n |      :class:`StreamingQuery` instances active on `this` context.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Notes\n |      -----\n |      This API is evolving.\n |      \n |      Returns\n |      -------\n |      :class:`StreamingQueryManager`\n |  \n |  udf\n |      Returns a :class:`UDFRegistration` for UDF registration.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Returns\n |      -------\n |      :class:`UDFRegistration`\n |  \n |  version\n |      The version of Spark on which this application is running.\n |      \n |      .. versionadded:: 2.0\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n |      Builder for :class:`SparkSession`.\n |  \n |  \n |  __annotations__ = {'_activeSession': typing.ClassVar[typing.Optional[F...\n |  \n |  builder = <pyspark.sql.session.SparkSession.Builder object>\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pyspark.sql.pandas.conversion.SparkConversionMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# ctrl + space ==> inteligence"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d4b4e5ea-c7b0-4a28-bffe-aec9429c0039","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["read a file which is save on DBFS with multiple extension"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d143c8df-906c-4551-80d9-44325b5cfc79","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df= spark.read.csv(path = 'dbfs:/user/hive/warehouse/DataFrame1.csv')\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b0c5277-f54f-4feb-ba3e-3fa2d00e47c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["20","rahul","2002","200","M","8000"],["60","rasul","2014","500","M","5000"],["30","ravi","2010","100","M","6000"],["10","raj","1999","100","M","2000"],["40","raja","2010","400",null,"7000"],["50","rama","2008","400",null,"1000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"string\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"},{"name":"_c2","type":"\"string\"","metadata":"{}"},{"name":"_c3","type":"\"string\"","metadata":"{}"},{"name":"_c4","type":"\"string\"","metadata":"{}"},{"name":"_c5","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>20</td><td>rahul</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>60</td><td>rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>30</td><td>ravi</td><td>2010</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>50</td><td>rama</td><td>2008</td><td>400</td><td>null</td><td>1000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df= spark.read.csv('dbfs:/user/hive/warehouse/DataFrame1.csv',header=True,inferSchema=True)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8bbf4363-3d28-43b5-bfb2-eb4411e3e085","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"20","type":"\"integer\"","metadata":"{}"},{"name":"rahul","type":"\"string\"","metadata":"{}"},{"name":"2002","type":"\"integer\"","metadata":"{}"},{"name":"200","type":"\"integer\"","metadata":"{}"},{"name":"M","type":"\"string\"","metadata":"{}"},{"name":"8000","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>20</th><th>rahul</th><th>2002</th><th>200</th><th>M</th><th>8000</th></tr></thead><tbody></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- 20: integer (nullable = true)\n |-- rahul: string (nullable = true)\n |-- 2002: integer (nullable = true)\n |-- 200: integer (nullable = true)\n |-- M: string (nullable = true)\n |-- 8000: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["print(df)\n# display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98c31ec9-0746-4cee-a9cf-4e4558c80e1b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["DataFrame[20: int, rahul: string, 2002: int, 200: int, M: string, 8000: int]\n"]}],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d13fb5c-3bad-441a-9b5c-d9a680796ddb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+----+---+---+----+\n| 20|rahul|2002|200|  M|8000|\n+---+-----+----+---+---+----+\n+---+-----+----+---+---+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df= spark.read.csv(\"dbfs:/user/hive/warehouse/DataFrame1.csv\")\ndf.head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"86e37f74-e623-4e25-a2e3-8c9b6221e313","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[13]: [Row(_c0='20', _c1='rahul', _c2='2002', _c3='200', _c4='M', _c5='8000'),\n Row(_c0='60', _c1='rasul', _c2='2014', _c3='500', _c4='M', _c5='5000'),\n Row(_c0='30', _c1='ravi', _c2='2010', _c3='100', _c4='M', _c5='6000'),\n Row(_c0='10', _c1='raj', _c2='1999', _c3='100', _c4='M', _c5='2000'),\n Row(_c0='40', _c1='raja', _c2='2010', _c3='400', _c4=None, _c5='7000')]"]}],"execution_count":0},{"cell_type":"code","source":["df= spark.read.csv(\"dbfs:/user/hive/warehouse/DataFrame1.csv\",inferSchema=True)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c429e12-bd43-4d3a-87b9-1405755c1ac6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[20,"rahul",2002,200,"M",8000],[60,"rasul",2014,500,"M",5000],[30,"ravi",2010,100,"M",6000],[10,"raj",1999,100,"M",2000],[40,"raja",2010,400,null,7000],[50,"rama",2008,400,null,1000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"integer\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"},{"name":"_c2","type":"\"integer\"","metadata":"{}"},{"name":"_c3","type":"\"integer\"","metadata":"{}"},{"name":"_c4","type":"\"string\"","metadata":"{}"},{"name":"_c5","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>20</td><td>rahul</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>60</td><td>rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>30</td><td>ravi</td><td>2010</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>50</td><td>rama</td><td>2008</td><td>400</td><td>null</td><td>1000</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _c0: integer (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: integer (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df= spark.read.format('parquet').load(\"dbfs:/user/hive/warehouse/df1\")\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c561fd3-b52c-429c-a76b-f39c139a4600","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[20,"rahul","2002","200","M",8000],[60,"rasul","2014","500","M",5000],[30,"ravi","2010","100","M",6000],[10,"raj","1999","100","M",2000],[40,"raja","2010","400",null,7000],[50,"rama","2008","400",null,1000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"employee_id","type":"\"long\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"doj","type":"\"string\"","metadata":"{}"},{"name":"employee_dept_id","type":"\"string\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"},{"name":"salary","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>employee_dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>20</td><td>rahul</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>60</td><td>rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>30</td><td>ravi</td><td>2010</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>50</td><td>rama</td><td>2008</td><td>400</td><td>null</td><td>1000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df= spark.read.format('csv').option(key='header',value=True).load(path='dbfs:/user/hive/warehouse/DataFrame1.csv')\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ba253b5-9b8c-471e-a2f5-fe00bbf85807","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+----+---+---+----+\n| 20|rahul|2002|200|  M|8000|\n+---+-----+----+---+---+----+\n+---+-----+----+---+---+----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["read multiple file at once with same schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"92315235-7b30-4db7-b326-7694fea26bd7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df= spark.read.csv(path=['dbfs:/user/hive/warehouse/DataFrame1.csv/part-00000-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-256-1-c000.csv','dbfs:/user/hive/warehouse/DataFrame1.csv/part-00001-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-257-1-c000.csv','dbfs:/user/hive/warehouse/DataFrame1.csv/part-00002-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-258-1-c000.csv','dbfs:/user/hive/warehouse/DataFrame1.csv/part-00003-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-259-1-c000.csv','dbfs:/user/hive/warehouse/DataFrame1.csv/part-00005-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-261-1-c000.csv'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1037c8b6-1cc9-48c6-b668-b6cca1cb053a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.display()\ndf.printSchema()\n# it show all the datatype of column is string "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2535c8dd-c702-4372-ad31-bc1ccdbb97ed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["20","rahul","2002","200","M","8000"],["30","ravi","2010","100","M","6000"],["10","raj","1999","100","M","2000"],["40","raja","2010","400",null,"7000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"string\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"},{"name":"_c2","type":"\"string\"","metadata":"{}"},{"name":"_c3","type":"\"string\"","metadata":"{}"},{"name":"_c4","type":"\"string\"","metadata":"{}"},{"name":"_c5","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>20</td><td>rahul</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>30</td><td>ravi</td><td>2010</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010</td><td>400</td><td>null</td><td>7000</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df= spark.read.csv(path=['dbfs:/user/hive/warehouse/DataFrame1.csv/part-00000-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-256-1-c000.csv','dbfs:/user/hive/warehouse/DataFrame1.csv/part-00001-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-257-1-c000.csv','dbfs:/user/hive/warehouse/DataFrame1.csv/part-00002-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-258-1-c000.csv','dbfs:/user/hive/warehouse/DataFrame1.csv/part-00003-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-259-1-c000.csv','dbfs:/user/hive/warehouse/DataFrame1.csv/part-00005-tid-5023340192568268437-efea0d2d-02aa-4125-8c61-c638be8af5e7-261-1-c000.csv'],inferSchema=True)\ndf.display()\ndf.printSchema()\n# now the datatype of each column is correct we can custom datatype with the help of StructType and StructField"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"715ed403-c61d-4973-a1ef-99e51cbabed5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[20,"rahul",2002,200,"M",8000],[30,"ravi",2010,100,"M",6000],[10,"raj",1999,100,"M",2000],[40,"raja",2010,400,null,7000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"integer\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"},{"name":"_c2","type":"\"integer\"","metadata":"{}"},{"name":"_c3","type":"\"integer\"","metadata":"{}"},{"name":"_c4","type":"\"string\"","metadata":"{}"},{"name":"_c5","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>20</td><td>rahul</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>30</td><td>ravi</td><td>2010</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010</td><td>400</td><td>null</td><td>7000</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _c0: integer (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: integer (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["use * to read all csv file at once"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7fc0c09-1957-4018-bc27-ed971401e90e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df= spark.read.csv('dbfs:/user/hive/warehouse/DataFrame1.csv/*.csv',inferSchema=True)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"babd8f2f-bb6d-49d7-97c3-a40c296b2e69","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["_c0","_c1","_c2","_c3","_c4","_c5"],["20","rahul","2002-01-01","200","M","8000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["60","rasul","2014-01-01","500","M","5000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["20","rahul","2002-01-01","200","M","8000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["60","rasul","2014-01-01","500","M","5000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["30","ravi","2010-01-01","100","M","6000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["30","ravi","2010-01-01","100","M","6000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["10","raj","1999-01-01","100","M","2000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["40","raja","2010-01-01","400",null,"7000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["50","rama","2008-01-01","400",null,"1000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["10","raj","1999-01-01","100","M","2000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["40","raja","2010-01-01","400",null,"7000"],["_c0","_c1","_c2","_c3","_c4","_c5"],["50","rama","2008-01-01","400",null,"1000"],["20","rahul","2002","200","M","8000"],["60","rasul","2014","500","M","5000"],["30","ravi","2010","100","M","6000"],["10","raj","1999","100","M","2000"],["40","raja","2010","400",null,"7000"],["50","rama","2008","400",null,"1000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"string\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"},{"name":"_c2","type":"\"string\"","metadata":"{}"},{"name":"_c3","type":"\"string\"","metadata":"{}"},{"name":"_c4","type":"\"string\"","metadata":"{}"},{"name":"_c5","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>_c0</td><td>_c1</td><td>_c2</td><td>_c3</td><td>_c4</td><td>_c5</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr><tr><td>20</td><td>rahul</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>60</td><td>rasul</td><td>2014</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>30</td><td>ravi</td><td>2010</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>50</td><td>rama</td><td>2008</td><td>400</td><td>null</td><td>1000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["custom schema"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d02d79e2-d793-494d-9768-8a51a7e6aaf1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import *\ndata = [(1,'umair'),(2,\"pihu\")]\nschema = StructType().add(field='id',data_type=IntegerType())\\\n         .add(field='name',data_type=StringType())\ndf= spark.createDataFrame(data,schema)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9804ce6-c2fb-4a17-b3fb-73d037020212","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"umair"],[2,"pihu"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>umair</td></tr><tr><td>2</td><td>pihu</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\ndata = [{'id':1,'name':\"umair\"},{'id':2,'name':\"pihu\"},{\"id\":3,'name':\"priyanka\"}]\nschema= StructType([StructField(name ='id',dataType=IntegerType()),StructField(name= 'name',dataType=StringType())])\ndf = spark.createDataFrame(data,schema)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a424c7a-a784-49d0-993b-7f9dda300476","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"umair"],[2,"pihu"],[3,"priyanka"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"integer\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>umair</td></tr><tr><td>2</td><td>pihu</td></tr><tr><td>3</td><td>priyanka</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema= StructType([StructField(name='_c0',dataType=IntegerType()),StructField(name='_c1',dataType=StringType()),\\\n                   StructField(name='_c2',dataType=DateType()),StructField(name='_c3',dataType=IntegerType()),\\\n                  StructField(name='_c4',dataType=StringType()),StructField(name=\"_c5\",dataType=IntegerType())])\ndf= spark.read.csv('dbfs:/user/hive/warehouse/DataFrame1.csv/*.csv',schema=schema)\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cceee98d-cd7d-4ad2-8f49-bfee9674a493","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[null,"_c1",null,null,"_c4",null],[20,"rahul","2002-01-01",200,"M",8000],[null,"_c1",null,null,"_c4",null],[60,"rasul","2014-01-01",500,"M",5000],[null,"_c1",null,null,"_c4",null],[20,"rahul","2002-01-01",200,"M",8000],[null,"_c1",null,null,"_c4",null],[60,"rasul","2014-01-01",500,"M",5000],[null,"_c1",null,null,"_c4",null],[30,"ravi","2010-01-01",100,"M",6000],[null,"_c1",null,null,"_c4",null],[30,"ravi","2010-01-01",100,"M",6000],[null,"_c1",null,null,"_c4",null],[10,"raj","1999-01-01",100,"M",2000],[null,"_c1",null,null,"_c4",null],[40,"raja","2010-01-01",400,null,7000],[null,"_c1",null,null,"_c4",null],[50,"rama","2008-01-01",400,null,1000],[null,"_c1",null,null,"_c4",null],[10,"raj","1999-01-01",100,"M",2000],[null,"_c1",null,null,"_c4",null],[40,"raja","2010-01-01",400,null,7000],[null,"_c1",null,null,"_c4",null],[50,"rama","2008-01-01",400,null,1000],[20,"rahul","2002-01-01",200,"M",8000],[60,"rasul","2014-01-01",500,"M",5000],[30,"ravi","2010-01-01",100,"M",6000],[10,"raj","1999-01-01",100,"M",2000],[40,"raja","2010-01-01",400,null,7000],[50,"rama","2008-01-01",400,null,1000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"integer\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"},{"name":"_c2","type":"\"date\"","metadata":"{}"},{"name":"_c3","type":"\"integer\"","metadata":"{}"},{"name":"_c4","type":"\"string\"","metadata":"{}"},{"name":"_c5","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType().add(field=\"_c0\",data_type=IntegerType())\\\n                          .add(field=\"_c1\",data_type=StringType())\\\n                          .add(field=\"_c2\",data_type=DateType())\\\n                          .add(field=\"_c3\",data_type=IntegerType())\\\n                          .add(field=\"_c4\",data_type=StringType())\\\n                          .add(field=\"_c5\",data_type=IntegerType())\ndf=spark.read.format('csv').load(\"dbfs:/user/hive/warehouse/DataFrame1.csv/*.csv\",schema=schema)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"578015bf-0b16-48b6-81d6-db7791e226c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[null,"_c1",null,null,"_c4",null],[20,"rahul","2002-01-01",200,"M",8000],[null,"_c1",null,null,"_c4",null],[60,"rasul","2014-01-01",500,"M",5000],[null,"_c1",null,null,"_c4",null],[20,"rahul","2002-01-01",200,"M",8000],[null,"_c1",null,null,"_c4",null],[60,"rasul","2014-01-01",500,"M",5000],[null,"_c1",null,null,"_c4",null],[30,"ravi","2010-01-01",100,"M",6000],[null,"_c1",null,null,"_c4",null],[30,"ravi","2010-01-01",100,"M",6000],[null,"_c1",null,null,"_c4",null],[10,"raj","1999-01-01",100,"M",2000],[null,"_c1",null,null,"_c4",null],[40,"raja","2010-01-01",400,null,7000],[null,"_c1",null,null,"_c4",null],[50,"rama","2008-01-01",400,null,1000],[null,"_c1",null,null,"_c4",null],[10,"raj","1999-01-01",100,"M",2000],[null,"_c1",null,null,"_c4",null],[40,"raja","2010-01-01",400,null,7000],[null,"_c1",null,null,"_c4",null],[50,"rama","2008-01-01",400,null,1000],[20,"rahul","2002-01-01",200,"M",8000],[60,"rasul","2014-01-01",500,"M",5000],[30,"ravi","2010-01-01",100,"M",6000],[10,"raj","1999-01-01",100,"M",2000],[40,"raja","2010-01-01",400,null,7000],[50,"rama","2008-01-01",400,null,1000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"integer\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"},{"name":"_c2","type":"\"date\"","metadata":"{}"},{"name":"_c3","type":"\"integer\"","metadata":"{}"},{"name":"_c4","type":"\"string\"","metadata":"{}"},{"name":"_c5","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType().add(field=\"_c0\",data_type=IntegerType())\\\n                    .add(field=\"_c1\",data_type=StringType())\\\n                    .add(field=\"_c2\",data_type=DateType())\\\n                    .add(field='_c3',data_type=IntegerType())\\\n                    .add(field=\"_c4\",data_type=StringType())\\\n                    .add(field=\"_c5\",data_type=IntegerType())\ndf= spark.read.load(path=\"dbfs:/user/hive/warehouse/DataFrame1.csv/*.csv\",schema=schema,format='csv')\n\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e6bb3bda-2f5e-4da2-9838-ba82607e54fd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[null,"_c1",null,null,"_c4",null],[20,"rahul","2002-01-01",200,"M",8000],[null,"_c1",null,null,"_c4",null],[60,"rasul","2014-01-01",500,"M",5000],[null,"_c1",null,null,"_c4",null],[20,"rahul","2002-01-01",200,"M",8000],[null,"_c1",null,null,"_c4",null],[60,"rasul","2014-01-01",500,"M",5000],[null,"_c1",null,null,"_c4",null],[30,"ravi","2010-01-01",100,"M",6000],[null,"_c1",null,null,"_c4",null],[30,"ravi","2010-01-01",100,"M",6000],[null,"_c1",null,null,"_c4",null],[10,"raj","1999-01-01",100,"M",2000],[null,"_c1",null,null,"_c4",null],[40,"raja","2010-01-01",400,null,7000],[null,"_c1",null,null,"_c4",null],[50,"rama","2008-01-01",400,null,1000],[null,"_c1",null,null,"_c4",null],[10,"raj","1999-01-01",100,"M",2000],[null,"_c1",null,null,"_c4",null],[40,"raja","2010-01-01",400,null,7000],[null,"_c1",null,null,"_c4",null],[50,"rama","2008-01-01",400,null,1000],[20,"rahul","2002-01-01",200,"M",8000],[60,"rasul","2014-01-01",500,"M",5000],[30,"ravi","2010-01-01",100,"M",6000],[10,"raj","1999-01-01",100,"M",2000],[40,"raja","2010-01-01",400,null,7000],[50,"rama","2008-01-01",400,null,1000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_c0","type":"\"integer\"","metadata":"{}"},{"name":"_c1","type":"\"string\"","metadata":"{}"},{"name":"_c2","type":"\"date\"","metadata":"{}"},{"name":"_c3","type":"\"integer\"","metadata":"{}"},{"name":"_c4","type":"\"string\"","metadata":"{}"},{"name":"_c5","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>null</td><td>_c1</td><td>null</td><td>null</td><td>_c4</td><td>null</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr><tr><td>20</td><td>rahul</td><td>2002-01-01</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>60</td><td>rasul</td><td>2014-01-01</td><td>500</td><td>M</td><td>5000</td></tr><tr><td>30</td><td>ravi</td><td>2010-01-01</td><td>100</td><td>M</td><td>6000</td></tr><tr><td>10</td><td>raj</td><td>1999-01-01</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>40</td><td>raja</td><td>2010-01-01</td><td>400</td><td>null</td><td>7000</td></tr><tr><td>50</td><td>rama</td><td>2008-01-01</td><td>400</td><td>null</td><td>1000</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _c0: integer (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: date (nullable = true)\n |-- _c3: integer (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import *\nhelp(DataFrameWriter)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a6dc6c19-e84c-4b47-9d5b-ffffa5f0cfda","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Help on class DataFrameWriter in module pyspark.sql.readwriter:\n\nclass DataFrameWriter(OptionUtils)\n |  DataFrameWriter(df: 'DataFrame')\n |  \n |  Interface used to write a :class:`DataFrame` to external storage systems\n |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n |  to access this.\n |  \n |  .. versionadded:: 1.4\n |  \n |  Method resolution order:\n |      DataFrameWriter\n |      OptionUtils\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, df: 'DataFrame')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Buckets the output by the given columns. If specified,\n |      the output is laid out on the file system similar to Hive's bucketing scheme,\n |      but with a different bucket hash function and is not compatible with Hive's bucketing.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Parameters\n |      ----------\n |      numBuckets : int\n |          the number of buckets to save\n |      col : str, list or tuple\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Notes\n |      -----\n |      Applicable for file-based data sources in combination with\n |      :py:meth:`DataFrameWriter.saveAsTable`.\n |      \n |      Examples\n |      --------\n |      >>> (df.write.format('parquet')  # doctest: +SKIP\n |      ...     .bucketBy(100, 'year', 'month')\n |      ...     .mode(\"overwrite\")\n |      ...     .saveAsTable('bucketed_table'))\n |  \n |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n |      \n |      .. versionadded:: 2.0.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n |              exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n |          in the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      >>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n |  \n |  format(self, source: str) -> 'DataFrameWriter'\n |      Specifies the underlying output data source.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      source : str\n |          string, name of the data source, e.g. 'json', 'parquet'.\n |      \n |      Examples\n |      --------\n |      >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))\n |  \n |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None\n |      Inserts the content of the :class:`DataFrame` to the specified table.\n |      \n |      It requires that the schema of the :class:`DataFrame` is the same as the\n |      schema of the table.\n |      \n |      Parameters\n |      ----------\n |      overwrite : bool, optional\n |          If true, overwrites existing data. Disabled by default\n |      \n |      Notes\n |      -----\n |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores\n |      the column names and just uses position-based resolution.\n |      \n |      .. versionadded:: 1.4\n |  \n |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None\n |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      table : str\n |          Name of the table in the external database.\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      properties : dict\n |          a dictionary of JDBC database connection arguments. Normally at\n |          least properties \"user\" and \"password\" with their corresponding values.\n |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n |          in the version you use.\n |      \n |          .. # noqa\n |      \n |      Notes\n |      -----\n |      Don't create too many partitions in parallel on a large cluster;\n |      otherwise Spark might crash your external database systems.\n |  \n |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None\n |      Saves the content of the :class:`DataFrame` in JSON format\n |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n |      specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n |          in the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\n |  \n |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'\n |      Specifies the behavior when data or table already exists.\n |      \n |      Options include:\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Examples\n |      --------\n |      >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n |  \n |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds an output option for the underlying data source.\n |      \n |      .. versionadded:: 1.5\n |  \n |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n |      Adds output options for the underlying data source.\n |      \n |      .. versionadded:: 1.4\n |  \n |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n |      \n |      .. versionadded:: 1.5.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n |          in the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      >>> orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\n |      >>> orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))\n |  \n |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : str or list, optional\n |          names of partitioning columns\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n |          in the version you use.\n |      \n |          .. # noqa\n |      \n |      Examples\n |      --------\n |      >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n |  \n |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'\n |      Partitions the output by the given columns on the file system.\n |      \n |      If specified, the output is laid out on the file system similar\n |      to Hive's partitioning scheme.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      cols : str or list\n |          name of columns\n |      \n |      Examples\n |      --------\n |      >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n |  \n |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the contents of the :class:`DataFrame` to a data source.\n |      \n |      The data source is specified by the ``format`` and a set of ``options``.\n |      If ``format`` is not specified, the default data source configured by\n |      ``spark.sql.sources.default`` will be used.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Parameters\n |      ----------\n |      path : str, optional\n |          the path in a Hadoop supported file system\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          specifies the behavior of the save operation when data already exists.\n |      \n |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n |          * ``overwrite``: Overwrite existing data.\n |          * ``ignore``: Silently ignore this operation if data already exists.\n |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n |      partitionBy : list, optional\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |      \n |      Examples\n |      --------\n |      >>> df.write.mode(\"append\").save(os.path.join(tempfile.mkdtemp(), 'data'))\n |  \n |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n |      Saves the content of the :class:`DataFrame` as the specified table.\n |      \n |      In the case the table already exists, behavior of this function depends on the\n |      save mode, specified by the `mode` function (default to throwing an exception).\n |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n |      the same as that of the existing table.\n |      \n |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n |      * `overwrite`: Overwrite existing data.\n |      * `error` or `errorifexists`: Throw an exception if data already exists.\n |      * `ignore`: Silently ignore this operation if data already exists.\n |      \n |      .. versionadded:: 1.4.0\n |      \n |      Notes\n |      -----\n |      When `mode` is `Append`, if there is an existing table, we will use the format and\n |      options of the existing table. The column order in the schema of the :class:`DataFrame`\n |      doesn't need to be same as that of the existing table. Unlike\n |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n |      column names to find the correct column positions.\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          the table name\n |      format : str, optional\n |          the format used to save\n |      mode : str, optional\n |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n |      partitionBy : str or list\n |          names of partitioning columns\n |      **options : dict\n |          all other string options\n |  \n |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n |      Sorts the output in each bucket by the given columns on the file system.\n |      \n |      .. versionadded:: 2.3.0\n |      \n |      Parameters\n |      ----------\n |      col : str, tuple or list\n |          a name of a column, or a list of names.\n |      cols : str\n |          additional names (optional). If `col` is a list it should be empty.\n |      \n |      Examples\n |      --------\n |      >>> (df.write.format('parquet')  # doctest: +SKIP\n |      ...     .bucketBy(100, 'year', 'month')\n |      ...     .sortBy('day')\n |      ...     .mode(\"overwrite\")\n |      ...     .saveAsTable('sorted_bucketed_table'))\n |  \n |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None\n |      Saves the content of the DataFrame in a text file at the specified path.\n |      The text files will be encoded as UTF-8.\n |      \n |      .. versionadded:: 1.6.0\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          the path in any Hadoop supported file system\n |      \n |      Other Parameters\n |      ----------------\n |      Extra options\n |          For the extra options, refer to\n |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n |          in the version you use.\n |      \n |          .. # noqa\n |      \n |      The DataFrame must have only one column that is of string type.\n |      Each row becomes a new line in the output file.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from OptionUtils:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["write df on DBFS"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"52ce5de5-2a5f-47b8-8dae-4dfffb23eacd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# 1\nfrom pyspark.sql import DataFrameWriter\ndf.write.option('header',True).csv(path='dbfs:/user/hive/warehouse/DataFrame1.csv/option.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f52945bc-c271-4982-8f04-24cb94b8d62b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-2684074137462186>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# 1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDataFrameWriter\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'header'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'dbfs:/user/hive/warehouse/DataFrame1.csv/option.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1238\u001B[0m             \u001B[0mlineSep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlineSep\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1239\u001B[0m         )\n\u001B[0;32m-> 1240\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1241\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1242\u001B[0m     def orc(\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: path dbfs:/user/hive/warehouse/DataFrame1.csv/option.csv already exists.","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: path dbfs:/user/hive/warehouse/DataFrame1.csv/option.csv already exists.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-2684074137462186>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# 1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDataFrameWriter\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'header'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'dbfs:/user/hive/warehouse/DataFrame1.csv/option.csv'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1238\u001B[0m             \u001B[0mlineSep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlineSep\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1239\u001B[0m         )\n\u001B[0;32m-> 1240\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1241\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1242\u001B[0m     def orc(\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: path dbfs:/user/hive/warehouse/DataFrame1.csv/option.csv already exists."]}}],"execution_count":0},{"cell_type":"code","source":["# 2\ndf.write.options(header='True',delimator=',').csv('dbfs:/user/hive/warehouse/DataFrame2.csv/options.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6c9cd56-85bf-4083-8ea5-06cf4d9b3a31","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# 3\ndf.write.csv(path='dbfs:/user/hive/warehouse/DataFrame3.csv/write_csv.csv',header=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28d6a5d0-0865-450f-bcfd-ab7a5a94a32f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# 4\ndf.write.csv(path='dbfs:/user/hive/warehouse/DataFrame4.csv/write_csv_mode.csv',header=True,mode='overwrite')\n\n# option of mode\n# append: Append contents of this :class:`DataFrame` to existing data.\n# overwrite: Overwrite existing data.\n# ignore: Silently ignore this operation if data already exists.\n# error or errorifexists (default case): Throw an exception if data already \\ exists."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff472ae8-f88d-44eb-b69f-369a30dc565b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["help(df.write.csv)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a73d0f5a-90c1-4574-b8a3-60f0c4563177","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df.write.mode('overwrite').csv('dbfs:/user/hive/warehouse/DataFrame4.csv/write_csv_mode.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bdd9d5b-c577-48e8-8f7c-6e37aa28770e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df.write.format('csv').mode('overwrite').save('dbfs:/user/hive/warehouse/DataFrame4.csv/write_csv_mode.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c8e60e9e-9be1-4667-826f-8154b7bba0ad","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# df=spark.read('org.apache.spark.sql.Json').load('dbfs:/user/hive/warehouse/DataFrame4.csv/write_csv_mode.csv')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab674837-330f-4f74-8381-289df0697f7a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# for json file if line is having multiline\ndf=spark.read.json('dbfs:/FileStore/sample_json_file.json',multiLine=True)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"141f17e6-5725-4225-b4f8-d6dd87af38ed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# read multiple file at once which is having same schema\ndf=spark.read.json(path=['dbfs:/FileStore/sample_json_file.json','dbfs:/FileStore/sample_json_file-1.json'],multiLine=True)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48b3c379-9f74-4eb8-9704-9898d8542896","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df=spark.read.json('dbfs:/FileStore/*.json',multiLine=True)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e3021ff-acaa-496b-ac3a-f53f8a1f477a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["write DataFrame Into JSON"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1b7406c-0c18-4186-b346-e024d03db11f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.write.json(path='dbfs:/user/hive/warehouse/DataFrame4.json/write_json.json')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7dc00197-f6e9-4242-80be-519a24989889","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df.write.parquet(path='dbfs:/user/hive/warehouse/DataFrame4.parquet/write_parqute.parquet')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24a5eed2-c94b-466e-89b4-c8a6a7126ef3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df=spark.read.format('parquet').load('dbfs:/user/hive/warehouse/DataFrame4.parquet/write_parqute.parquet')\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"982dc90b-76b5-46ff-9199-eade1cd33bef","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["how to read excel file using pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60b3b9d2-9071-4581-9c66-ee89d2b4c4a8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.read.format(\"com.crealytics.spark.excel\")\\\n     .option(\"dataAddress\", \"'Sheet1'!\")\\\n     .option(\"header\", \"true\")\\\n     .option(\"inferSchema\", \"true\")\\\n     .load(\"dbfs:/FileStore/Book1.xlsx\")\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e822cc9a-611c-4f66-a342-71c7d5aa9eee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# import com.crealytics.spark.excel\ndf = spark.read.format('com.crealytics.spark.excel').option('header',True).option('inferSchema',True).load('dbfs:/FileStore/Book1.xlsx')\ndf.show(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5fef3114-431b-4203-acad-25b1fb8f6b13","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["show function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"751d33c0-fb19-4a0a-bc8d-f318bdb2f28b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["help(df.show)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"950ede98-b120-4f1c-b4fe-ed8531206f48","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["data=[(\"mohammad umair mohammad FArooque\",25,\"81K\",\"fair\",5.6),(\"jafar khan sarfaraz khan\",26,\"20k\",\"dark\",5.6)]\nschema=[\"name\",\"age\",\"salary\",\"color\",\"height\"]\ndf= spark.createDataFrame(data,schema)\ndf.show()\n# only 20 charactor by default in each cell"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2122dad-537a-4dbe-915c-1ebca293d6f1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df.show(n=1)\n# n= no of row to show"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d77be58c-cb4e-42b2-9f1b-225bb135ce0d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=False)\n# it go beyond there limit and show full characture in each cell"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bff5856f-aea3-447e-a192-1d006a528420","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=10)\n# it show maximum 10 characture at max"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7251ae36-fe5e-4054-8a8e-efe3a7b5b1a5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["how to change the datatype of column in dataframe \nwithColumn"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5fd00d9-b47c-473f-a979-f010b4a65ab8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.show()\ndf.printSchema()\n# we need to change age datatype from long to integerTyoe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc229642-d568-4cc6-ab50-56c1cd71b3bb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf = df.withColumn(colName='age',col=col('age').cast('Integer'))\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24cbf078-8b4a-4f6c-9890-090cf5b2eaef","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df.withColumn('new_age', df.age + 2).show()\n# df.show()\n# help(df.withColumn)\n# df.withColumn(nem_col_name, expression).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5062802e-7a80-4d00-8f82-1cf7c0e221d3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, lit"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02149d00-b579-4779-a22c-4d35c70709fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# create new column with expression \ndata=[('umair',90000,\"khamgaon\"),('jafar',85000,'pune')]\nschema=['name','salary','city']\ndf= spark.createDataFrame(data,schema)\ndf.show()\ndf.withColumn('updated_salary',col(\"salary\")*1.2).show()\n# df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70eedae1-3c8f-49d4-bfc2-57f81127a6e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# create new column with custom value\ndata=[('umair',90000,\"khamgaon\"),('jafar',85000,'pune')]\nschema=['name','salary','city']\ndf= spark.createDataFrame(data,schema)\ndf.show()\ndf.withColumn('qaulification',lit(\"engg\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"306cb6c3-39e9-48a2-b0b1-00c14e4907e0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["rename of columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14929fe6-260c-4e11-8836-f75b9f4278d8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.display()\ndf = df.withColumnRenamed(\"salary\",\"salary_in_usd\")\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ae08729-ab8c-44c7-b49e-cf587131d42d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["example of complex columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49c26582-d57d-4c77-8f66-438aff64067a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType\ndata =[(1,('Umair','Memon'),25,45000),(2,('Zeeshan','Shaik'),27,56000)]\nstructname=StructType([StructField(name=\"First_name\",dataType=StringType()),\\\n                      StructField(name=\"Last_name\",dataType=StringType())])\nschema = StructType([StructField(name=\"Sr.no\",dataType=IntegerType()),\\\n                    StructField(name=\"Name\",dataType=structname),\\\n                    StructField(name=\"Age\",dataType=IntegerType()),\\\n                    StructField(name=\"Salary\",dataType=IntegerType())])\ndf=spark.createDataFrame(data,schema)\n\ndf.show()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"034253c8-d580-4cf3-9cbe-edcd9c735aa7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\ndata =[(1,('Umair','Memon'),25,45000),(2,('Zeeshan','Shaik'),27,56000)]\nschema = StructType([StructField(\"Sr.no\",IntegerType()),\\\n                    StructField('Name',ArrayType(StringType())),\\\n                    StructField(\"Age\",IntegerType()),\\\n                    StructField('Salary',IntegerType())])\ndf=spark.createDataFrame(data,schema)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58faef12-9042-4a11-b120-764a8abdda7b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["Create new column from array column"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"55554305-3750-4fec-8d6d-ea6ae9d4cf53","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.show()\ndf = df.withColumn(\"first_name\",df.Name[0])\ndf = df.withColumn(\"last_name\",df.Name[1])\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c61a4dd1-4c9b-4abe-8e53-7f516846e819","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["Create Array column from exiting column"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8c1d0f9-ea17-4d39-814e-2772f28867b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, array\n# array and ArrayType both are different\ndata=[(1,2),(3,4)]\nschema=['num1','num2']\ndf= spark.createDataFrame(data,schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71954cf0-5b2e-45c1-bcbc-694b03648cc1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df=df.withColumn('numbers',array(col(\"num1\"),col(\"num2\")))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dbcea4cc-4100-465e-94c8-3ded5b5630ec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["common fuction used with array\nexplode(), Split(), array(), array_contain()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"403aa505-1503-4f91-9ae7-e3cf43c239fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Split()\n# it is use to make array of common sep value \n\nfrom pyspark.sql.functions import split\n# 1\nprint('row data form')\ndata = [(1,'umair',\"ml,nlp\"),(1,'zeeshan',\"dl,ml\")]\nschema=['sr','name','skill']\ndf= spark.createDataFrame(data,schema)\ndf.show()\n# 2\nprint('in control')\ndf= df.withColumn('skillarray',split(col(\"skill\"),','))\ndf.show()\n# 3\nprint(\"final result\")\ndf=df.withColumn('primary_skill',df.skillarray[0])\ndf=df.withColumn('secondary_skill',df.skillarray[1])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b7a7bd17-daaf-4746-8dd9-dbeffaa72b0d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# Array_contains()\nprint('''it is a sql function is used to check if array column contain a value , return null if the array is null true if the array contain the value and false otherwise''')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07049f67-24ad-488a-8111-23c84a976be6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#  it only work on array type column\n\nfrom pyspark.sql.functions import array_contains\ndf.withColumn(\"has_ml\",array_contains(col('skillarray'),'ml')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"08220505-55bc-4f7e-ad25-76eb9cd3714f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# split()\nprint('''it is a sql function return an array type after splitting the string column by delimiter''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ce2d094-4092-4dc5-9304-255516255372","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#explode()\nprint('''use explode function to create a new row for each element in the given array column''')\n\n# array()\nprint('''use array function to create anew arrey column by merging the data from multiple columns''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12303c02-5310-443b-a2d3-120e332d7588","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["MapType()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8a436a9-c306-4ebd-8833-87e208b8a96b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# MapType() ==>dictionary\nprint('''pyspark MapType is used to represent map key-value pair similar to python dictionary''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b30bdcef-9341-4c1c-b193-a53c2e6c3bb5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, MapType\n\ndata=[('umair',{\"hair\":\"black\",'eye':'brown'}),('zeeshan',{\"hair\":\"long\",\"eye\":\"black\"})]\nschema=StructType([StructField(\"name\",StringType()),StructField(\"characturestics\",MapType(StringType(),StringType()))])\ndf=spark.createDataFrame(data,schema)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89941e98-ecb4-458c-bd75-fe6798a32d14","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# we can create new column for each property of MapType\ndf=df.withColumn('hair',df.characturestics['hair'])\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8acd0c6e-3ed3-4af1-a95d-31d032e230b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df=df.withColumn('eye',df.characturestics['eye'])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d25fed6-50ed-4c33-ba4a-ee9b53321556","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# we can also use it\ndf=df.withColumn('eye_color',df.characturestics.getField('eye'))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"400fc6e3-f543-4448-b91d-7cf7838cf2e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df=df.withColumn('hair_color',df.characturestics.getItem('hair'))\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d369c2ec-c8bc-4030-b23f-4381386f1825","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# function of MapType  \n# explode()\n# map_keys()\n# map_values()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b04095c7-623d-47e4-ba78-55c66a329cf2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# example\nfrom pyspark.sql.functions import explode, map_keys, map_values\ndf1=df.select('name',explode(df.characturestics))\ndf1.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"08616805-98ad-4168-8a0f-783a914faa53","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df1=df.select('name',explode('characturestics').alias('part','color'))\ndf1.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc7a874d-d234-46e1-97ad-f1330204d93f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df1=df.select(explode('characturestics').alias('part','color'))\ndf1.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7347a354-0434-48ce-b97a-08f70e0eafbe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df1=df.withColumn('keys',map_keys(df.characturestics))\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6874085f-ac14-4221-94f1-40b3dbcbd7a0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df1= df1.withColumn(\"values\",map_values(df.characturestics))\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73633e7a-e319-4d1e-a102-3a3d89964f69","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["Row()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82083d95-6a40-4f59-a030-fc0ff645d271","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print('''pyspark.sql.Row which is represented as a record row in dataframe one can create row object by using named argument or create a custom row like class''')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d96bfe2-d69a-4619-893e-34737cacfbf0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nrow= Row('umair','pihu')\nprint(row[1]+' is the gf of '+row[0])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"058118a0-0113-4f52-b5f8-35427f31e2ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["# using name arrguments\nrow= Row(name='umair',salary=\"80000\")\nprint(row.name+' has the salary '+row.salary)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0828d80f-081a-4b84-aaef-60303d7b7a94","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["row1= Row(name='pihu',salary='120000')\nrow2= Row(name='umair',salary=\"80000\")\ndata=(row1,row2)\ndf=spark.createDataFrame(data)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"620ec88c-9223-47cf-bf90-19010c2f0af4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["person=Row('name','age')\nperson1=person('Umair',25)\nperson2=person('Pihu',26)\n\nprint(person1.name)\nprint(person2.name)\n\ndata=[person1,person2]\ndf=spark.createDataFrame(data,person)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ef4c990-343f-4fd3-a478-248666961b98","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["we can create neasted Structtype also using Row()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b74cd19b-abd6-4f21-aaa1-885e22556079","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[Row(name='umair',prop=Row(hair='Black',eye='Black')),\\\n     Row(name='zeeshan',prop=Row(hair='brown',eye='black'))]\ndf=spark.createDataFrame(data)\ndf.display()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15a3bf5c-07cb-4b2f-b525-d57601b41032","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["Column"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28397425-c66b-45dd-93a1-5f1d2ac3814f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print('pyspark column class represent a single column in a data frame')\nprint()\nprint('pyspark.sql.column class provides several functions to work with dataframe to manipulate the column values evalute the boolean expression to fillter row(), retrive a value or part of a value from a data frame column')\nprint()\nprint('one of the simpliest way to create column class object is by using pyspark lit() sql function')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f9fc62d-11e6-4a11-95db-47baefa6e594","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["pyspark column class represent a single column in a data frame\n\npyspark.sql.column class provides several functions to work with dataframe to manipulate the column values evalute the boolean expression to fillter row(), retrive a value or part of a value from a data frame column\n\none of the simpliest way to create column class object is by using pyspark lit() sql function\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lit\ncol1= lit('abcd')\nprint(type(col1))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b42dd7c8-45f3-4768-b801-2087eb619876","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<class 'pyspark.sql.column.Column'>\n"]}],"execution_count":0},{"cell_type":"code","source":["print(\"\"\"you can access column multiple ways from DataFrame\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3bc47221-7889-4336-bf85-874ad4ef77bb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data=[('umair','male',25),('zeeshan','male',27)]\nschema=['name','gender','age']\ndf=spark.createDataFrame(data,schema)\n\nfrom pyspark.sql.functions import col\ndf.select(df.gender).show()\ndf.select(col('gender')).show()\ndf.select(df['gender']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d6e4b40-e9c7-4079-9e5a-13e347e65599","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+\n|gender|\n+------+\n|  male|\n|  male|\n+------+\n\n+------+\n|gender|\n+------+\n|  male|\n|  male|\n+------+\n\n+------+\n|gender|\n+------+\n|  male|\n|  male|\n+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Accessing Struct column \nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\ndata=[('umair','male',('Black','Black')),\\\n     ('zeeshan','male',('brown','black'))]\nStructname=StructType([StructField('hair',StringType()),\\\n                      StructField('eye',StringType())])\n\nschema=StructType([StructField('name',StringType()),\\\n                  StructField('gender',StringType()),\\\n                  StructField('prop',Structname)])\ndf=spark.createDataFrame(data,schema)\ndf.select(df.prop.eye).show()\ndf.select(df.prop.hair).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f91b91c5-f2c3-4d3e-b5bb-68b62ca2685f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------+\n|prop.eye|\n+--------+\n|   Black|\n|   black|\n+--------+\n\n+---------+\n|prop.hair|\n+---------+\n|    Black|\n|    brown|\n+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\ndata = [\n        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n        ]\n\nfrom pyspark.sql.types import StructType,StructField, StringType        \nschema = StructType([\n    StructField('name', StructType([\n         StructField('firstname', StringType(), True),\n         StructField('middlename', StringType(), True),\n         StructField('lastname', StringType(), True)\n         ])),\n     StructField('state', StringType(), True),\n     StructField('gender', StringType(), True)\n     ])\ndf2 = spark.createDataFrame(data = data, schema = schema)\ndf2.printSchema()\ndf2.show(truncate=False) # shows all columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9c86776-47f6-43c2-8827-f1a8fed8353c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+----------------------+-----+------+\n|name                  |state|gender|\n+----------------------+-----+------+\n|{James, null, Smith}  |OH   |M     |\n|{Anna, Rose, }        |NY   |F     |\n|{Julia, , Williams}   |OH   |F     |\n|{Maria, Anne, Jones}  |NY   |M     |\n|{Jen, Mary, Brown}    |NY   |M     |\n|{Mike, Mary, Williams}|OH   |M     |\n+----------------------+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(\"name\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0be828a-b902-4b78-974d-b97865003e35","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------------+\n|name                  |\n+----------------------+\n|{James, null, Smith}  |\n|{Anna, Rose, }        |\n|{Julia, , Williams}   |\n|{Maria, Anne, Jones}  |\n|{Jen, Mary, Brown}    |\n|{Mike, Mary, Williams}|\n+----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c506076-74e5-484a-9d1d-b59649172466","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|James    |Smith   |\n|Anna     |        |\n|Julia    |Williams|\n|Maria    |Jones   |\n|Jen      |Brown   |\n|Mike     |Williams|\n+---------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(df2.name.firstname).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"206813ef-adb1-4c3c-9042-ba7efc9d25a4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------+\n|name.firstname|\n+--------------+\n|         James|\n|          Anna|\n|         Julia|\n|         Maria|\n|           Jen|\n|          Mike|\n+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(df2.name.firstname,df2.name.lastname).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"afefbc6d-e542-41b0-b86e-9daf4a2b72a3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------+-------------+\n|name.firstname|name.lastname|\n+--------------+-------------+\n|         James|        Smith|\n|          Anna|             |\n|         Julia|     Williams|\n|         Maria|        Jones|\n|           Jen|        Brown|\n|          Mike|     Williams|\n+--------------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(col('name.firstname')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5822cb85-4b8b-4868-9632-935f63d9707c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+\n|firstname|\n+---------+\n|    James|\n|     Anna|\n|    Julia|\n|    Maria|\n|      Jen|\n|     Mike|\n+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(df2['name']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b44440e-2b9c-41a5-be80-6f4bf7e6fb8e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|                name|\n+--------------------+\n|{James, null, Smith}|\n|      {Anna, Rose, }|\n| {Julia, , Williams}|\n|{Maria, Anne, Jones}|\n|  {Jen, Mary, Brown}|\n|{Mike, Mary, Will...|\n+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(df2['name.firstname']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c15b23e2-278c-40fe-9b7e-da8c3b1b98e5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+\n|firstname|\n+---------+\n|    James|\n|     Anna|\n|    Julia|\n|    Maria|\n|      Jen|\n|     Mike|\n+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df2.select(\"name.*\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2fb2d9e3-282d-48a0-959c-98551073f519","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----------+--------+\n|firstname|middlename|lastname|\n+---------+----------+--------+\n|James    |null      |Smith   |\n|Anna     |Rose      |        |\n|Julia    |          |Williams|\n|Maria    |Anne      |Jones   |\n|Jen      |Mary      |Brown   |\n|Mike     |Mary      |Williams|\n+---------+----------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["print('with lit we can create new column with custom value')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44823452-6878-4859-9956-0908f8b07f94","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["with lit we can create new column with custom value\n"]}],"execution_count":0},{"cell_type":"code","source":["df1=df2.withColumn('country',lit('USA'))\ndf1.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6211ec15-61a1-4ab2-af41-c0fe8a351d04","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[["James",null,"Smith"],"OH","M","USA"],[["Anna","Rose",""],"NY","F","USA"],[["Julia","","Williams"],"OH","F","USA"],[["Maria","Anne","Jones"],"NY","M","USA"],[["Jen","Mary","Brown"],"NY","M","USA"],[["Mike","Mary","Williams"],"OH","M","USA"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"firstname\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"middlename\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"lastname\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"state","type":"\"string\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"},{"name":"country","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>state</th><th>gender</th><th>country</th></tr></thead><tbody><tr><td>List(James, null, Smith)</td><td>OH</td><td>M</td><td>USA</td></tr><tr><td>List(Anna, Rose, )</td><td>NY</td><td>F</td><td>USA</td></tr><tr><td>List(Julia, , Williams)</td><td>OH</td><td>F</td><td>USA</td></tr><tr><td>List(Maria, Anne, Jones)</td><td>NY</td><td>M</td><td>USA</td></tr><tr><td>List(Jen, Mary, Brown)</td><td>NY</td><td>M</td><td>USA</td></tr><tr><td>List(Mike, Mary, Williams)</td><td>OH</td><td>M</td><td>USA</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# when() and otherwise()\n# it is similar to sql case when executes sequence of expressions until it matches the condition and return a value when match\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa295075-d17f-412f-a8d2-6e3e6980921f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import when \ndf1= df2.select(df2.name,df2.gender,when(condition=df2.gender=='M',value='Male')\\\n              .when(condition=df2.gender=='F',value='Female')\\\n              .otherwise('unknown').alias('gender-'))\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f5b1b9f-64a1-4092-9472-a187f128b081","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+------+-------+\n|                name|gender|gender-|\n+--------------------+------+-------+\n|{James, null, Smith}|     M|   Male|\n|      {Anna, Rose, }|     F| Female|\n| {Julia, , Williams}|     F| Female|\n|{Maria, Anne, Jones}|     M|   Male|\n|  {Jen, Mary, Brown}|     M|   Male|\n|{Mike, Mary, Will...|     M|   Male|\n+--------------------+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# function of column of dataframe\n# .alias() .asc() .desc() .cast() .like()\n# functions on column of dataframe in pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee5266ae-efc2-4f1b-ad88-5827d656ce8e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# rename the column of dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60212fbd-554d-40fc-bb8e-f8d627a02a2f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[(1,'Umair','M',200),(2,'zee',\"M\",300),(3,'jafar','M',250),(4,\"pihu\",\"F\",500)]\nschema=['id','name','gender','marks']\ndf=spark.createDataFrame(data,schema)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac706b64-c600-4d14-8f66-ca93e58d4031","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"Umair","M",200],[2,"zee","M",300],[3,"jafar","M",250],[4,"pihu","F",500]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"},{"name":"marks","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>gender</th><th>marks</th></tr></thead><tbody><tr><td>1</td><td>Umair</td><td>M</td><td>200</td></tr><tr><td>2</td><td>zee</td><td>M</td><td>300</td></tr><tr><td>3</td><td>jafar</td><td>M</td><td>250</td></tr><tr><td>4</td><td>pihu</td><td>F</td><td>500</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df1=df.select(df.id.alias('student_id'),df.name.alias('student_name')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1bb635de-ee4e-4f8b-b1f6-3e7916d42ac4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------------+\n|student_id|student_name|\n+----------+------------+\n|         1|       Umair|\n|         2|         zee|\n|         3|       jafar|\n|         4|        pihu|\n+----------+------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# asc() and desc()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2d5594c-a036-4762-ba74-91d526be36db","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#  sort columns ascending and descending order"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18352c75-1387-4e19-9312-db955af78b70","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data= [(1,'umair','M',200),(2,'faizan','M',400),(3,'vinayak','M',300)]\nschema=['id','name','gender','salary']\ndf=spark.createDataFrame(data,schema)\ndf.sort(df.name.asc()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6374ee6-2f0e-4955-8b87-24dd92c82c15","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  2| faizan|     M|   400|\n|  1|  umair|     M|   200|\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.sort(df.salary.desc()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de91060c-6a5f-4572-917e-ea995b3d658b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  2| faizan|     M|   400|\n|  3|vinayak|     M|   300|\n|  1|  umair|     M|   200|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.sort(df.name.desc()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3002080-29cd-444e-98d4-cebd4d094c07","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  3|vinayak|     M|   300|\n|  1|  umair|     M|   200|\n|  2| faizan|     M|   400|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df=df.sort(df.name.desc())\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a96b408c-7527-4987-860d-fd1c1b90505f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  3|vinayak|     M|   300|\n|  1|  umair|     M|   200|\n|  2| faizan|     M|   400|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# cast()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46a8e344-4da6-4c7e-8270-96d6343bc9fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# use to convert the data type of column in data frame\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"650ff2ce-7ee4-4735-aa90-057174ff5715","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"544d1392-1fd4-411e-b95e-30dc3fbf2cb0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# we need to change the datatype of salary and id as integer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5f737974-bd4c-49c3-9428-c04624d83e97","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1=(df.select(df.name,df.gender,df.id.cast('int'),df.salary.cast('int')))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11d141d3-2ce9-4a63-a276-a2f3413712d8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1.printSchema()\n# help(df.select(df.id.cast))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48b019ee-ed54-46ef-848b-19afe07bfdf8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- salary: integer (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# like()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9aa0857f-90f7-4678-b538-7014c02cad74","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# similary to sql like expression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f871a2a-3958-471d-a361-4b7e01931695","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"123fb525-4643-493f-aa36-a4e03c4ef45c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  1|  umair|     M|   200|\n|  2| faizan|     M|   400|\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.filter(df.name.like('u%')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00847daf-30f2-46de-b4f0-652d0d5f570d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+\n| id| name|gender|salary|\n+---+-----+------+------+\n|  1|umair|     M|   200|\n+---+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.filter(df.name=='umair').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f76d5eed-9aae-465d-bd68-ed2db20548c1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+\n| id| name|gender|salary|\n+---+-----+------+------+\n|  1|umair|     M|   200|\n+---+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.filter(df.name.like('%n')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24f3dcf4-d548-4d3c-a320-4772cab81dd0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+------+------+\n| id|  name|gender|salary|\n+---+------+------+------+\n|  2|faizan|     M|   400|\n+---+------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.filter(df.gender=='M').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7adc3110-8345-41d7-8af7-0e5f82bca93c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  1|  umair|     M|   200|\n|  2| faizan|     M|   400|\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.filter(df.id>=2).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"144ab099-abe1-40b7-affb-e49277cd00b4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  2| faizan|     M|   400|\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.filter(df.id<=2).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"65fe0bf6-85ed-4c25-951d-292913480b29","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+------+------+\n| id|  name|gender|salary|\n+---+------+------+------+\n|  1| umair|     M|   200|\n|  2|faizan|     M|   400|\n+---+------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# Filter() and Where()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"683d9352-9133-48d6-adf1-017411032709","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# pyspark filter() function is used to filter the rows from DataFrame based on the given condition or sql expression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f43a390-62b4-47a1-abfe-1883a2d7bbb0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# you can also use where() clause instead of the filter if you are coming from an sql background both of this functions operate exactly same"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cfbc8d52-f1d9-4787-85b2-f01c05fd6b8f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.where(df.gender=='M').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df3a3282-90ca-412a-bddd-9117691e9284","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  1|  umair|     M|   200|\n|  2| faizan|     M|   400|\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.filter(df.gender=='M').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"799b85f6-8ea7-4403-876d-5667c875bc89","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  1|  umair|     M|   200|\n|  2| faizan|     M|   400|\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.filter((df.salary==200) & (df.gender=='M')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"201257a0-cb02-4695-982c-21b178030f5f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+\n| id| name|gender|salary|\n+---+-----+------+------+\n|  1|umair|     M|   200|\n+---+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.where((df.salary==300) & (df.gender=='M')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"694f07e2-c4a5-4a4e-850b-5bd44cd7d306","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.where((df.name=='umair') | (df.salary==500)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b7285e44-749e-472c-8a2b-ade4e38f89b5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+\n| id| name|gender|salary|\n+---+-----+------+------+\n|  1|umair|     M|   200|\n+---+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# distinct() and dropDuplicates() in pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dccfefad-de9f-4c08-975d-5670610514d9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# pyspark distinct()function is usede to remove the duplicates rows ( all column)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0f65654-d3c2-4d9c-82fd-160e7ecb9690","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# dropDuplicates() is used to drop rows based on selected (one or multiple) columns."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b0d135b-76a8-4102-8162-a067f823c821","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# so basically using these functions we can get distinct rows."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c45afb2e-f1cf-4560-8a22-331a1325908c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data=[(1, 'umair', 'M', 200), (2, 'faizan', 'M', 400), (3, 'vinayak', 'M', 300),(1, 'umair', 'M', 200),(3, 'vinayak', 'M', 300)]\nschema=['id','name','gender','salary']\ndf=spark.createDataFrame(data,schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68e68bf7-9d97-4692-851f-5c674928690d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.dropDuplicates().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc73eec1-9d21-4cc9-8d48-289993e73cd2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  1|  umair|     M|   200|\n|  2| faizan|     M|   400|\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.dropDuplicates(['gender']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bb514c58-1733-4ce4-bd9f-9a72b2a66448","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+\n| id| name|gender|salary|\n+---+-----+------+------+\n|  1|umair|     M|   200|\n+---+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.dropDuplicates(['gender','salary']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0edd5cbd-4f64-4aeb-88dc-c82c6160dd4c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  1|  umair|     M|   200|\n|  3|vinayak|     M|   300|\n|  2| faizan|     M|   400|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.distinct().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1344db59-c398-48fd-80aa-8ede3ba2faa0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+------+\n| id|   name|gender|salary|\n+---+-------+------+------+\n|  1|  umair|     M|   200|\n|  2| faizan|     M|   400|\n|  3|vinayak|     M|   300|\n+---+-------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# it does'nt work\n# df.distinct(['gender']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7923bea-c862-4357-8b99-3a2b844082cc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# groupBy()\nsimilar to sql groupby clause pyspark groupby() function is used to collect the identical data into groups on dataframe and perform count, sum, min, max, function on the grouped data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"265a261d-5aad-4824-8fc4-462948ac6c07","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# df = spark.createDataFrame(data,schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c359034-6c13-48d0-b5c2-07f299406419","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data=[(1, 'umair', 'M', 600), (2, 'faizan', 'M', 400), (3, 'vinayak', 'M', 900),(1, 'umair', 'M', 100),(3, 'vinayak', 'M', 400)]\nschema=['id','name','gender','salary']\ndf=spark.createDataFrame(data,schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6e63e32-db3f-4743-867d-db4a24043caf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.groupBy('name').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"285e1363-604e-48e7-b7c1-09779e7c8374","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-----+\n|   name|count|\n+-------+-----+\n|  umair|    2|\n| faizan|    1|\n|vinayak|    2|\n+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy('name').max().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c3b3b7f2-c8d4-4547-9f1d-a2a4f331f6f5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-------+-----------+\n|   name|max(id)|max(salary)|\n+-------+-------+-----------+\n|  umair|      1|        600|\n| faizan|      2|        400|\n|vinayak|      3|        900|\n+-------+-------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy('name').min('salary').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d957593-2254-4961-94d9-bafe5a8bfe3d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-----------+\n|   name|min(salary)|\n+-------+-----------+\n|  umair|        100|\n| faizan|        400|\n|vinayak|        400|\n+-------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy('name').avg('salary').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e82178fa-41e4-45fe-a5c5-8a7d4fb33bee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-----------+\n|   name|avg(salary)|\n+-------+-----------+\n|  umair|      350.0|\n| faizan|      400.0|\n|vinayak|      650.0|\n+-------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy('name','gender').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab31d3dc-fc51-4ecd-ad45-b608eaa2b56c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+-----+\n|   name|gender|count|\n+-------+------+-----+\n|  umair|     M|    2|\n| faizan|     M|    1|\n|vinayak|     M|    2|\n+-------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# Agre function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"939b09ac-ce1f-40ce-a125-9d7fc8931f20","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# groupBy().agg() function in pyspark\n\n# pyspark groupBy() agg() is used to calculate more than one aggregate (multiple aggregates) at a time on grouped DataFrame\n\nfrom pyspark.sql.functions import count, min, max ,sum\ndf.groupBy('name').agg(sum('salary').alias('income')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4e07790f-1bba-4862-a17f-7f9142c4a601","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+\n|   name|income|\n+-------+------+\n|  umair|   700|\n| faizan|   400|\n|vinayak|  1300|\n+-------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy('id').agg(count('*').alias('no of condidate'),\\\nmin('salary').alias('min salary'),\\\nmax('salary').alias('max salary')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ae928972-8f9b-4d6a-b9b8-ab086e581f38","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------------+----------+----------+\n| id|no of condidate|min salary|max salary|\n+---+---------------+----------+----------+\n|  1|              2|       100|       600|\n|  2|              1|       400|       400|\n|  3|              2|       400|       900|\n+---+---------------+----------+----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# unionByName() in pysaprk"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e92990fc-4a14-4754-9a98-2deccadfbfec","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# UnionByName() lets you to merge/ union teo DataFrame with a different number of column (different schema) by passing allowMissingColumn with the value True"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d5fb7277-a035-4944-b0d1-76682cc35977","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data1 = [(1,'umair','male')]\nschema1=['id','name','gender']\n\ndata2= [(2,'pihu','female')]\nschema2=['id','name','salary']\n\ndf1=spark.createDataFrame(data1,schema1)\ndf2=spark.createDataFrame(data2,schema2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"adc63a3b-2057-4dc8-85ec-14f660285192","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1.union(df2).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98d8b576-bc88-42ba-acff-87a7e5075339","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+\n| id| name|gender|\n+---+-----+------+\n|  1|umair|  male|\n|  2| pihu|female|\n+---+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data1 = [(1,'umair','male')]\nschema1=['id','name','gender']\n\ndata2= [(2,'pihu',2000)]\nschema2=['id','name','salary']\n\ndf1=spark.createDataFrame(data1,schema1)\ndf2=spark.createDataFrame(data2,schema2)\n\ndf1.unionByName(allowMissingColumns=True,other=df2).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3161f97f-d845-4b64-b025-ca8f222e90ee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+\n| id| name|gender|salary|\n+---+-----+------+------+\n|  1|umair|  male|  null|\n|  2| pihu|  null|  2000|\n+---+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data1 = [(1,'umair','male')]\nschema1=['id','name','gender']\n\ndata2= [(2,'pihu','female')]\nschema2=['id','name','gender']\n\ndf1=spark.createDataFrame(data1,schema1)\ndf2=spark.createDataFrame(data2,schema2)\n\ndf1.unionByName(other=df2).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6aa22092-40bd-4ec6-8278-51d8fda7603c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+\n| id| name|gender|\n+---+-----+------+\n|  1|umair|  male|\n|  2| pihu|female|\n+---+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# select() function in pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f09a10e5-6e06-4caf-ac84-bf9b32eaf849","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# select() function is used to select single multiple, column by index all  column from the list and the nested column from a DataFrame "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"d3b17266-c7cf-4c61-ba03-44e00eed114d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = [(1,'umair','male',20000),(2,'pihu','female',28000),(3,'annu','female',25000)]\nschema=['id','name','gender','salary']\ndf=spark.createDataFrame(data,schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2bc6558-ca99-4f4b-928e-b5838da0409a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# select single or multiple columns\ndf.select('id','name').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6762df10-3946-4f96-bfad-dc894d2063ea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+\n| id| name|\n+---+-----+\n|  1|umair|\n|  2| pihu|\n|  3| annu|\n+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# 1\ndf.select('name','salary').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e7c2183-7bfb-412b-9a43-eb4e4e6951dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------+\n| name|salary|\n+-----+------+\n|umair| 20000|\n| pihu| 28000|\n| annu| 25000|\n+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# 2\ndf.select(df.name,df.salary).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"92783011-9183-43f1-8f9d-e965a4698c2c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------+\n| name|salary|\n+-----+------+\n|umair| 20000|\n| pihu| 28000|\n| annu| 25000|\n+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# 3\ndf.select(df['name'],df['salary']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d743d2cb-8f86-4daf-9704-487943429a0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------+\n| name|salary|\n+-----+------+\n|umair| 20000|\n| pihu| 28000|\n| annu| 25000|\n+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select(['name','salary']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d4f1bea-2acd-4d55-b7fe-1a478ba0e910","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------+\n| name|salary|\n+-----+------+\n|umair| 20000|\n| pihu| 28000|\n| annu| 25000|\n+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# using col() function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b37599ce-f414-4ccf-b92a-fe8bea275dfc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf.select(col('id'),col('name')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d86f45cc-d63e-48c2-9a05-e2e5d23fbea4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+\n| id| name|\n+---+-----+\n|  1|umair|\n|  2| pihu|\n|  3| annu|\n+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# join() function in pyspark\ninner, left, right, full"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3d1a795-7f2f-4c82-8c0c-ce7db5da819e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["right=emp_df.join(dep_df, dep_df.id==emp_df.id, 'right')\nright.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70ef59dc-71c0-4d0f-869c-ac7385aab78c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+-----+------+----+---+-------+\n|  id| name|salary| age| id| stream|\n+----+-----+------+----+---+-------+\n|   1|umair|  2000|  25|  1|     IT|\n|   2| pihu|  2500|  25|  2|  civil|\n|   3| annu|  1500|  23|  3|finance|\n|null| null|  null|null|  6|    law|\n|null| null|  null|null|  5|     HR|\n+----+-----+------+----+---+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["left = emp_df.join(dep_df, dep_df.id==emp_df.id, 'left')\nleft.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90285450-b479-4468-818a-2733624740fb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+---+----+-------+\n| id|   name|salary|age|  id| stream|\n+---+-------+------+---+----+-------+\n|  1|  umair|  2000| 25|   1|     IT|\n|  2|   pihu|  2500| 25|   2|  civil|\n|  3|   annu|  1500| 23|   3|finance|\n|  4|zeeshan|  5500| 28|null|   null|\n|  7|  jafar|  1000| 26|null|   null|\n+---+-------+------+---+----+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["inner=emp_df.join(dep_df, emp_df.id==dep_df.id, 'inner')\ninner.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54cb046d-4940-42b2-b056-1f5b417af131","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+---+---+-------+\n| id| name|salary|age| id| stream|\n+---+-----+------+---+---+-------+\n|  1|umair|  2000| 25|  1|     IT|\n|  2| pihu|  2500| 25|  2|  civil|\n|  3| annu|  1500| 23|  3|finance|\n+---+-----+------+---+---+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data1=[(1,'umair',2000,25),(2,'pihu',2500,25),(3,'annu',1500,23),(4,'zeeshan',5500,28),(7,'jafar',1000,26)]\ndata2=[(1,'IT'),(2,'civil'),(3,'finance'),(6,'law'),(5,'HR')]\n\nschema1=['id','name','salary','age']\nschema2=['id','stream']\n\nemp_df=spark.createDataFrame(data1,schema1)\ndep_df=spark.createDataFrame(data2,schema2)\n\nemp_df.show()\ndep_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a05674e2-ceb5-46e9-ae88-2106606bc617","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+---+\n| id|   name|salary|age|\n+---+-------+------+---+\n|  1|  umair|  2000| 25|\n|  2|   pihu|  2500| 25|\n|  3|   annu|  1500| 23|\n|  4|zeeshan|  5500| 28|\n|  7|  jafar|  1000| 26|\n+---+-------+------+---+\n\n+---+-------+\n| id| stream|\n+---+-------+\n|  1|     IT|\n|  2|  civil|\n|  3|finance|\n|  6|    law|\n|  5|     HR|\n+---+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# join() is like sql join we can combine columns from different dataframe based on condition it support all basic join types such as inner, left, outer, right outer, left anti, left semi, cross, self"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71643395-c19c-4ddb-942d-28a4703e939c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["full=emp_df.join(dep_df, dep_df.id==emp_df.id, 'full')\nfull.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7311dc3a-c016-4fa3-8e51-9f06257eee5c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+-------+------+----+----+-------+\n|  id|   name|salary| age|  id| stream|\n+----+-------+------+----+----+-------+\n|   1|  umair|  2000|  25|   1|     IT|\n|   2|   pihu|  2500|  25|   2|  civil|\n|   3|   annu|  1500|  23|   3|finance|\n|   4|zeeshan|  5500|  28|null|   null|\n|null|   null|  null|null|   5|     HR|\n|null|   null|  null|null|   6|    law|\n|   7|  jafar|  1000|  26|null|   null|\n+----+-------+------+----+----+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["cross=emp_df.join(dep_df, dep_df.id==emp_df.id, 'cross')\ncross.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe267c0a-90de-4fc3-b3fc-1a81a2cd5e16","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+---+---+-------+\n| id| name|salary|age| id| stream|\n+---+-----+------+---+---+-------+\n|  1|umair|  2000| 25|  1|     IT|\n|  2| pihu|  2500| 25|  2|  civil|\n|  3| annu|  1500| 23|  3|finance|\n+---+-----+------+---+---+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# how : str, optional\n#         default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n#         ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n#         ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n#         ``anti``, ``leftanti`` and ``left_anti``."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5325e97-462d-4a64-a74e-c201f1bdf9ff","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cross=emp_df.join(dep_df, dep_df.id==emp_df.id, 'semi')\ncross.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"573b12fa-db9c-4133-b11d-dc3264eece37","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+---+\n| id| name|salary|age|\n+---+-----+------+---+\n|  1|umair|  2000| 25|\n|  2| pihu|  2500| 25|\n|  3| annu|  1500| 23|\n+---+-----+------+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["cross=emp_df.join(dep_df, dep_df.id==emp_df.id, 'anti')\ncross.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e68e01d5-0fa8-4171-8460-304fdfbda65e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+---+\n| id|   name|salary|age|\n+---+-------+------+---+\n|  4|zeeshan|  5500| 28|\n|  7|  jafar|  1000| 26|\n+---+-------+------+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["cross=emp_df.join(dep_df, dep_df.id==emp_df.id, 'full_outer')\ncross.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c2187590-4f82-46a7-86ee-d3de0d70cb5d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+-------+------+----+----+-------+\n|  id|   name|salary| age|  id| stream|\n+----+-------+------+----+----+-------+\n|   1|  umair|  2000|  25|   1|     IT|\n|   2|   pihu|  2500|  25|   2|  civil|\n|   3|   annu|  1500|  23|   3|finance|\n|   4|zeeshan|  5500|  28|null|   null|\n|null|   null|  null|null|   5|     HR|\n|null|   null|  null|null|   6|    law|\n|   7|  jafar|  1000|  26|null|   null|\n+----+-------+------+----+----+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# join() in pyspark continuation left semi, left anti, self\n\nleftsemi join is simmiler to inner join but get column only from left dataframe for matching rows\n\nleftanti opposite to leftsemi, it gets not matching rows from left dataframe \n\nself join, join data with same dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a41f78b-ac3b-49f5-93a5-6dd2fe473dfa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data1=[(1,'umair',2000,25),(2,'pihu',2500,25),(3,'annu',1500,23),(4,'zeeshan',5500,28),(7,'jafar',1000,26)]\ndata2=[(1,'IT'),(2,'civil'),(3,'finance'),(6,'law'),(5,'HR')]\n\nschema1=['id','name','salary','age']\nschema2=['id','stream']\n\nemp_df=spark.createDataFrame(data1,schema1)\ndep_df=spark.createDataFrame(data2,schema2)\n\nemp_df.show()\ndep_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c87baf39-e490-4962-8706-0a2c02c3ea1c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+---+\n| id|   name|salary|age|\n+---+-------+------+---+\n|  1|  umair|  2000| 25|\n|  2|   pihu|  2500| 25|\n|  3|   annu|  1500| 23|\n|  4|zeeshan|  5500| 28|\n|  7|  jafar|  1000| 26|\n+---+-------+------+---+\n\n+---+-------+\n| id| stream|\n+---+-------+\n|  1|     IT|\n|  2|  civil|\n|  3|finance|\n|  6|    law|\n|  5|     HR|\n+---+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["emp_df.join(dep_df, emp_df.id==dep_df.id, 'leftanti').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f214757-fc13-42e2-8721-0796b367ecec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+------+---+\n| id|   name|salary|age|\n+---+-------+------+---+\n|  4|zeeshan|  5500| 28|\n|  7|  jafar|  1000| 26|\n+---+-------+------+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["emp_df.join(dep_df, dep_df.id==emp_df.id, 'leftsemi').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2bc5b1ee-d83b-4cfb-9ffd-e76875e21f79","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+---+\n| id| name|salary|age|\n+---+-----+------+---+\n|  1|umair|  2000| 25|\n|  2| pihu|  2500| 25|\n|  3| annu|  1500| 23|\n+---+-----+------+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[(1,'umair',0),(2,'annu',1),(3,'pihu',2)]\nschema=['id','name','managerid']\ndf=spark.createDataFrame(data,schema)\ndf.show()\n\nemp = manager = df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5485932a-8bdf-4db7-aebd-aae9168c899e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+---------+\n| id| name|managerid|\n+---+-----+---------+\n|  1|umair|        0|\n|  2| annu|        1|\n|  3| pihu|        2|\n+---+-----+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf1= df.alias('emp').join(df.alias('manager'), emp.managerid==manager.id,'inner')\\\n.select(emp.name, manager.name.alias('manager_name')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6957d0e4-4d87-429a-9920-411e1ac2ec41","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-14271223519173>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'emp'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'manager'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanagerid\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0mmanager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'inner'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmanager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'manager_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mjoin\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   1623\u001B[0m                 \u001B[0mon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jseq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1624\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"how should be a string\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1625\u001B[0;31m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mother\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1626\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1627\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m:  Column managerid#942L, id#940L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        ","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>:  Column managerid#942L, id#940L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-14271223519173>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf1\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'emp'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'manager'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanagerid\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0mmanager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'inner'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmanager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'manager_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mjoin\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   1623\u001B[0m                 \u001B[0mon\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jseq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1624\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"how should be a string\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1625\u001B[0;31m             \u001B[0mjdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mother\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mon\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1626\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1627\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m:  Column managerid#942L, id#940L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        "]}}],"execution_count":0},{"cell_type":"code","source":["emp.join(manager, col(emp.managerid)==col(manager.id),'left')\\\n.select(col('emp.name'),col('manager.name').alias('manager_name')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"032bc351-6025-458d-abea-87c348ffa11a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-14271223519174>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmanager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanagerid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmanager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'left'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'emp.name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'manager.name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'manager_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36mcol\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[0mColumn\u001B[0m\u001B[0;34m<\u001B[0m\u001B[0;34m'x'\u001B[0m\u001B[0;34m>\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m     \"\"\"\n\u001B[0;32m--> 154\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_invoke_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    155\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36m_invoke_function\u001B[0;34m(name, *args)\u001B[0m\n\u001B[1;32m     85\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m     \u001B[0mjf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_jvm_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1312\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1313\u001B[0;31m         \u001B[0margs_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1314\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1315\u001B[0m         \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCALL_COMMAND_NAME\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_build_args\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1275\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1276\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1277\u001B[0;31m             \u001B[0;34m(\u001B[0m\u001B[0mnew_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1278\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1279\u001B[0m             \u001B[0mnew_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_get_args\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m   1262\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mconverter\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1263\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcan_convert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1264\u001B[0;31m                         \u001B[0mtemp_arg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1265\u001B[0m                         \u001B[0mtemp_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1266\u001B[0m                         \u001B[0mnew_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_collections.py\u001B[0m in \u001B[0;36mconvert\u001B[0;34m(self, object, gateway_client)\u001B[0m\n\u001B[1;32m    508\u001B[0m         \u001B[0mArrayList\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mJavaClass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"java.util.ArrayList\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    509\u001B[0m         \u001B[0mjava_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mArrayList\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 510\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0melement\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mobject\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    511\u001B[0m             \u001B[0mjava_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    512\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjava_list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    566\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 567\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Column is not iterable\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    568\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    569\u001B[0m     \u001B[0;31m# string methods\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: Column is not iterable","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: Column is not iterable","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-14271223519174>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmanager\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmanagerid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmanager\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'left'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'emp.name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'manager.name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'manager_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36mcol\u001B[0;34m(col)\u001B[0m\n\u001B[1;32m    152\u001B[0m     \u001B[0mColumn\u001B[0m\u001B[0;34m<\u001B[0m\u001B[0;34m'x'\u001B[0m\u001B[0;34m>\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m     \"\"\"\n\u001B[0;32m--> 154\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_invoke_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"col\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    155\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/functions.py\u001B[0m in \u001B[0;36m_invoke_function\u001B[0;34m(name, *args)\u001B[0m\n\u001B[1;32m     85\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m     \u001B[0mjf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_jvm_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1312\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1313\u001B[0;31m         \u001B[0margs_command\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1314\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1315\u001B[0m         \u001B[0mcommand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mproto\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCALL_COMMAND_NAME\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_build_args\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1275\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_build_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1276\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1277\u001B[0;31m             \u001B[0;34m(\u001B[0m\u001B[0mnew_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1278\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1279\u001B[0m             \u001B[0mnew_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m_get_args\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m   1262\u001B[0m                 \u001B[0;32mfor\u001B[0m \u001B[0mconverter\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconverters\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1263\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcan_convert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1264\u001B[0;31m                         \u001B[0mtemp_arg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconverter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1265\u001B[0m                         \u001B[0mtemp_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1266\u001B[0m                         \u001B[0mnew_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtemp_arg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_collections.py\u001B[0m in \u001B[0;36mconvert\u001B[0;34m(self, object, gateway_client)\u001B[0m\n\u001B[1;32m    508\u001B[0m         \u001B[0mArrayList\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mJavaClass\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"java.util.ArrayList\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    509\u001B[0m         \u001B[0mjava_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mArrayList\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 510\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0melement\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mobject\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    511\u001B[0m             \u001B[0mjava_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    512\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mjava_list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/column.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    565\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    566\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 567\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Column is not iterable\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    568\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    569\u001B[0m     \u001B[0;31m# string methods\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mTypeError\u001B[0m: Column is not iterable"]}}],"execution_count":0},{"cell_type":"markdown","source":["# pivot() function in pyspark \nit,s is use to rotate data in one column into multiple colmuns\nit is an aggregation where one of the grouping column values will be converted in individual columns."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"334f624a-320a-4367-992f-f56e3d59fdfb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[(1,'umair','male','IT'),(2,'pihu','female','civil'),(3,'annu','female','finance'),(4,'aish','female','HR')]\nschema=['id','name','gender','stream']\n\ndf=spark.createDataFrame(data,schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c77d86e4-4de4-4f65-9902-4137e34af30d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+-------+\n| id| name|gender| stream|\n+---+-----+------+-------+\n|  1|umair|  male|     IT|\n|  2| pihu|female|  civil|\n|  3| annu|female|finance|\n|  4| aish|female|     HR|\n+---+-----+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.dropna().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5911644d-a18e-487f-b59d-c1c6f11989c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+------+------+-----+\n| id|name|gender|salary|  dep|\n+---+----+------+------+-----+\n|  2|pihu|female|   200|civil|\n|  3|annu|female|   150|   HR|\n+---+----+------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df=spark.createDataFrame(rdd,['id','name'])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff6ce2f6-850d-4689-9616-bbf3e41a5bf4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+\n| id| name|\n+---+-----+\n|  1|umair|\n|  2| pihu|\n+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# to view global temp tables\nspark.catalog.listTables('global_temp')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11271f91-3bd3-4214-bc23-8b113a2bd819","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[74]: [Table(name='empGlobal', catalog=None, namespace=['global_temp'], description=None, tableType='TEMPORARY', isTemporary=True),\n Table(name='emplyees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy('Stream').pivot('gender',['male','female']).count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"306d2979-c54f-4a64-b003-e50efb9f6f95","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+----+------+\n| Stream|male|female|\n+-------+----+------+\n|finance|null|     1|\n|     HR|null|     1|\n|  civil|null|     1|\n|     IT|   1|  null|\n+-------+----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data= [(1,'umair','male',100,None),(2,'pihu',None,250,'civil'),(3,'annu','female',125,'finance')]\nschema=['id','name','gender','salary','dep']\n\ndf=spark.createDataFrame(data,schema)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e08c600-6513-4e3b-8f66-dc3c4f66ca21","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"umair","male",100,null],[2,"pihu",null,250,"civil"],[3,"annu","female",125,"finance"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"},{"name":"salary","type":"\"long\"","metadata":"{}"},{"name":"dep","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>gender</th><th>salary</th><th>dep</th></tr></thead><tbody><tr><td>1</td><td>umair</td><td>male</td><td>100</td><td>null</td></tr><tr><td>2</td><td>pihu</td><td>null</td><td>250</td><td>civil</td></tr><tr><td>3</td><td>annu</td><td>female</td><td>125</td><td>finance</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# these are similar to function in sql we define same logic in function and store them in dataframe and use them in querises.\n\n# simolar to that we can write our own custom logic in python function and register it with pyspark using udf() function "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00b77c0e-be91-4298-aee0-daeabe425890","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.fillna('data science').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c0ef810e-7f29-4980-8dbc-92a2ae198904","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+------------+\n| id| name|gender|salary|         dep|\n+---+-----+------+------+------------+\n|  1|umair|  male|   100|data science|\n|  2| pihu|female|   200|       civil|\n|  3| annu|female|   150|          HR|\n+---+-----+------+------+------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 40. UDF (user define functions)in pyspark\nwhat is udf\n\nwhy do we need it\n\nhow to create and use it"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39595d02-a988-4067-8ee6-9c3c1dbb91d3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# sample() function in pyspark\nto get the random sampling subset from the large dataset   \nuse fraction to indicate what percentage of data to return and seed value to make sure every time to get some random sample."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75476490-03ef-4dff-ac47-78511a9637ea","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[('umair','memon'),('pihu','patel')]\nrdd=spark.sparkContext.parallelize(data)\nrdd1=rdd.map(lambda x:x+(x[0]+' '+x[1],))\nprint(rdd1.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2263c4bd-29a2-4fde-9dfd-f2860bca57b2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[('umair', 'memon', 'umair memon'), ('pihu', 'patel', 'pihu patel')]\n"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy('name').pivot('Stream').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8538bffa-453f-4c2e-8910-6e665084221e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+----+-----+-------+\n| name|  HR|  IT|civil|finance|\n+-----+----+----+-----+-------+\n| pihu|null|null|    1|   null|\n| annu|null|null| null|      1|\n| aish|   1|null| null|   null|\n|umair|null|   1| null|   null|\n+-----+----+----+-----+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select('dep',expr(\"stack(2,'pottiya',female,'potte',male)as (sex,count)\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8013287-8e5c-4d5f-a44c-64e91fb6f889","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-------+-----+\n|    dep|    sex|count|\n+-------+-------+-----+\n|     IT|pottiya|    5|\n|     IT|  potte|    8|\n|payroll|pottiya|    2|\n|payroll|  potte|    3|\n|     HR|pottiya|    4|\n|     HR|  potte|    2|\n+-------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.createOrReplaceTempView('emplyees')\ndf1=spark.sql('select * from emplyees ')\ndf1.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ca7771f0-7d74-481a-b254-fbebadb40e7b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+-------+\n| id| name|gender|salary|    dep|\n+---+-----+------+------+-------+\n|  1|umair|  male|   100|   null|\n|  2| pihu|  null|   250|  civil|\n|  3| annu|female|   125|finance|\n+---+-----+------+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# stach() unpivot DataFrame in pyspark \nunpivot is rotating column in rows pyspark sql doesn't have unpivot function hence will use the stack() function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18f1f90c-2924-47bc-b17e-c8f6187a7860","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df1=df.groupBy('id').pivot('stream').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0b60fee-acd7-47b3-bf66-1df3d269f421","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+----+-----+-------+\n| id|  HR|  IT|civil|finance|\n+---+----+----+-----+-------+\n|  1|null|   1| null|   null|\n|  3|null|null| null|      1|\n|  2|null|null|    1|   null|\n|  4|   1|null| null|   null|\n+---+----+----+-----+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.na.fill('data science').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d52b6400-c36b-485f-81ba-24b4bc364499","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+------------+\n| id| name|gender|salary|         dep|\n+---+-----+------+------+------------+\n|  1|umair|  male|   100|data science|\n|  2| pihu|female|   200|       civil|\n|  3| annu|female|   150|          HR|\n+---+-----+------+------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import upper \ndef convertToUpper(df):\n    return df.withColumn('name',upper(df.name))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1772e6e1-9e4f-4359-b26c-4ad7920901d5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data= [(1,'umair','male',100,None),(2,'pihu',None,250,'civil'),(3,'annu','female',125,'finance')]\nschema=['id','name','gender','salary','dep']\n\ndf=spark.createDataFrame(data,schema)\ndf.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3e455d6-1c09-4750-a17b-306e2a988131","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"umair","male",100,null],[2,"pihu",null,250,"civil"],[3,"annu","female",125,"finance"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"gender","type":"\"string\"","metadata":"{}"},{"name":"salary","type":"\"long\"","metadata":"{}"},{"name":"dep","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>gender</th><th>salary</th><th>dep</th></tr></thead><tbody><tr><td>1</td><td>umair</td><td>male</td><td>100</td><td>null</td></tr><tr><td>2</td><td>pihu</td><td>null</td><td>250</td><td>civil</td></tr><tr><td>3</td><td>annu</td><td>female</td><td>125</td><td>finance</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(dataRows[0][0])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51efd2c8-a2b3-445f-b7c4-7fd11a14ea70","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["1\n"]}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect id, upper(name) from emplyees"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"481924d9-5687-4d07-af3c-601695c506c5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"UMAIR"],[2,"PIHU"],[3,"ANNU"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"upper(name)","type":"\"string\"","metadata":"{\"__autoGeneratedAlias\":\"true\"}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>upper(name)</th></tr></thead><tbody><tr><td>1</td><td>UMAIR</td></tr><tr><td>2</td><td>PIHU</td></tr><tr><td>3</td><td>ANNU</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df1=df.transform(convertToUpper).transform(doubleTheSalary)\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58529a7a-26d6-4ed1-89c1-d067a099c160","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+\n| id| name|Salary|\n+---+-----+------+\n|  1|UMAIR|   400|\n|  2| PIHU|   450|\n|  3| ANNU|   350|\n+---+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.dropna('all').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4946530b-3cdd-40f3-a0d1-41cc897f00bc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+-----+\n| id| name|gender|salary|  dep|\n+---+-----+------+------+-----+\n|  1|umair|  male|   100| null|\n|  2| pihu|female|   200|civil|\n|  3| annu|female|   150|   HR|\n+---+-----+------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn('totalpay',TotalPay(df.salary,df.bonus)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b870e719-4e12-4505-83ae-4a838d847be4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+-----+--------+\n| id| name|salary|bonus|totalpay|\n+---+-----+------+-----+--------+\n|  1|umair|  2000|  500|    2500|\n|  2| pihu|  3000|  200|    3200|\n|  3| annu|  2200|  100|    2300|\n+---+-----+------+-----+--------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# collect() \ncollect() retrive all elements in a dataframe as an array of row type to the driver node\n\ncoolect is the action hence it does not return a dataframe instead it return data in an array to the drive. once the data is in a array you can use python for loop to process it further.\n\ncollect() use it with small DataFrame with big DataFRame it may result in out of memeory error as it return entire data to sinle node (driver)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4fef96a3-b148-43f1-847b-8589680d0294","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# fill(), fillna() function in pyspark\nfillna( )  or DataFrame.Nafunctions.fill() is used to replace Null/none values on all or selected multiple daataframe columns with either zero(0), empty string, space, or any constrant literal values"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1bfa168-f13a-499f-b71d-517ec267a969","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# 37 pyspark.sql.functions.transformation() function in pyspark\n\nit is used to apply the transformation on a column of type array this function applies the specified transformation on every element of the array and return an object of ArrayType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dfc7acf2-708d-4aa5-a113-24a1b737d507","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\n\ndf.select('dep',expr(\"stack(2,'Male',male,'Female',female)as (Gender,Count)\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e32bcf7f-6163-415c-99bb-4b60e0e71897","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+-----+\n|    dep|Gender|Count|\n+-------+------+-----+\n|     IT|  Male|    8|\n|     IT|Female|    5|\n|payroll|  Male|    3|\n|payroll|Female|    2|\n|     HR|  Male|    2|\n|     HR|Female|    4|\n+-------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import transform\ndf.select('id','name',transform('skills',lambda x:upper(x)).alias('skills')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"000034e8-68bb-4180-85e0-30b04f2a74b2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+-----------------+\n| id| name|           skills|\n+---+-----+-----------------+\n|  1|umair|      [AZURE, ML]|\n|  2|  zee|[PYTHON, PYSPARK]|\n+---+-----+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[(1,'umair'),(2,'pihu')]\nrdd=spark.sparkContext.parallelize(data)\nprint(rdd.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32e21cc4-d6f5-44c2-97b0-fd78ab6e8818","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[(1, 'umair'), (2, 'pihu')]\n"]}],"execution_count":0},{"cell_type":"code","source":["df.show()\ndf.createOrReplaceGlobalTempView('empGlobal')\n\n# list tables from current session\nspark.catalog.listTables(spark.catalog.currentDatabase())\n\n# to view global temp tables\n# spark.catalog.listTables('global_temp')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebbb5784-4f7b-4dc5-ad8c-6cff380669e7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+-------+\n| id| name|gender|salary|    dep|\n+---+-----+------+------+-------+\n|  1|umair|  male|   100|   null|\n|  2| pihu|  null|   250|  civil|\n|  3| annu|female|   125|finance|\n+---+-----+------+------+-------+\n\nOut[73]: [Table(name='emplyees', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"]}],"execution_count":0},{"cell_type":"code","source":["df.fillna('unknown',['dep']).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7002155-2674-4f3a-b23b-7d6af68ba34c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+-------+\n| id| name|gender|salary|    dep|\n+---+-----+------+------+-------+\n|  1|umair|  male|   100|unknown|\n|  2| pihu|female|   200|  civil|\n|  3| annu|female|   150|     HR|\n+---+-----+------+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 39 createOrReplaceGlobalTempView() function in pyspark\n\nit is used  to create temp views or tables global when can be accessed across the session with in spark application \n\nto query these table we need append global_temp.<tablename>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c2adb9cf-1daf-4b10-8cd7-558558ca2844","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# 36 DataFrame.transform()\nit is used to chain the custom transformationand this function return the new dataframe after applying the specified transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"be6b77d3-cc6e-49cf-b2a7-968f9d7ba246","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def doubleTheSalary(df):\n    return df.withColumn('Salary',df.salary * 2)\n\ndoubleTheSalary(df).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15619522-7d04-4a7b-86db-32dee1b6921f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+\n| id| name|Salary|\n+---+-----+------+\n|  1|umair|   400|\n|  2| pihu|   450|\n|  3| annu|   350|\n+---+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["dataRows=df.collect()\nprint(dataRows)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ef4193a-b389-4c25-adf6-a1d0f0a4aefa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[Row(id=1, name='umair', gender='male', salary=100, dep=None), Row(id=2, name='pihu', gender=None, salary=250, dep='civil'), Row(id=3, name='annu', gender='female', salary=125, dep='finance')]\n"]}],"execution_count":0},{"cell_type":"code","source":["convertToUpper(df).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12177434-02e1-4f9b-81e1-bda618566506","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|UMAIR|   200|\n|  2| PIHU|   225|\n|  3| ANNU|   175|\n+---+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["def totalpay(s,b):\n    return s+b\n\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import IntegerType\n\nTotalPay = udf(lambda x,y :totalpay(x,y),IntegerType())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1c8ab71-d5f3-4d2c-93b0-f5d91f2710d7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.groupBy('stream').pivot('gender').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"114a4a7e-659f-435d-a93b-ad2d8f6417f1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+----+\n| stream|female|male|\n+-------+------+----+\n|finance|     1|null|\n|     HR|     1|null|\n|  civil|     1|null|\n|     IT|  null|   1|\n+-------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 42 map() transformation in pyspark\n\nit is RDD transformation used to apply function (lambda)on every element of RDD and return new RDD\n\nDataFRame does not have map() transformation to use with DataFrame you need to generate RDD first"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9746abad-11fa-4fd9-9f0e-652e3de7210a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[(1,'umair','male',100,None),(2,'pihu','female',200,'civil'),(3,'annu','female',150,'HR')]\nschema=['id','name','gender','salary','dep']\n\ndf=spark.createDataFrame(data,schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"57e3815d-d765-40db-8a29-52c95ab8af48","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+------+-----+\n| id| name|gender|salary|  dep|\n+---+-----+------+------+-----+\n|  1|umair|  male|   100| null|\n|  2| pihu|female|   200|civil|\n|  3| annu|female|   150|   HR|\n+---+-----+------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.dropna('any').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9f38fc4-3b0a-497d-b551-470342befec0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+------+------+-----+\n| id|name|gender|salary|  dep|\n+---+----+------+------+-----+\n|  2|pihu|female|   200|civil|\n|  3|annu|female|   150|   HR|\n+---+----+------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# scala code to get spark session id \n# %Scala"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"784e1394-7ee0-41b8-9341-81720256e522","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["UsageError: Line magic function `%Scala` not found.\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"<span class='ansi-red-fg'>UsageError</span>: Line magic function `%Scala` not found.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9518127-ad2d-4c32-aea1-60ce5bf3fa82","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[(1,'umair',200),(2,'pihu',225),(3,'annu',175)]\nschema =['id','name','salary']\n\ndf=spark.createDataFrame(data,schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d916013a-3b5f-4984-99b0-460522f31a22","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|umair|   200|\n|  2| pihu|   225|\n|  3| annu|   175|\n+---+-----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[(1,'umair',2000,500),(2,'pihu',3000,200),(3,\"annu\",2200,100)]\nschema=['id','name','salary','bonus']\ndf=spark.createDataFrame(data,schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39f95229-9763-43f8-87ed-7ad7561f2aa7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+-----+\n| id| name|salary|bonus|\n+---+-----+------+-----+\n|  1|umair|  2000|  500|\n|  2| pihu|  3000|  200|\n|  3| annu|  2200|  100|\n+---+-----+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df=rdd.toDF(['id','name'])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c69983e-1685-43d9-a3f3-f8cdbe0be064","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+\n| id| name|\n+---+-----+\n|  1|umair|\n|  2| pihu|\n+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[('IT',8,5),('payroll',3,2),('HR',2,4)]\nschema=['dep','male','female']\n\ndf=spark.createDataFrame(data,schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7a5f534c-8e34-4bc6-8215-1c0b9b78f61f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+----+------+\n|    dep|male|female|\n+-------+----+------+\n|     IT|   8|     5|\n|payroll|   3|     2|\n|     HR|   2|     4|\n+-------+----+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 41 convert RDD to DataFrame\n\nwhat is RDD  \n\nconvert RDD to DataFrame\n\nit is collection of objects similar to list in oython it is immutable and memory processing\n\nby using parallelize() function of sparkcontext you can create an RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23e9dd8b-34af-46f9-809b-710eaf1c9d59","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[(1,'umair',['azure','ML']),(2,'zee',['python','pyspark'])]\nschema=['id','name','skills']\n\ndf= spark.createDataFrame(data,schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bc8aa158-ebcf-433a-9982-31e0aaa809dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+-----------------+\n| id| name|           skills|\n+---+-----+-----------------+\n|  1|umair|      [azure, ML]|\n|  2|  zee|[python, pyspark]|\n+---+-----+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["def convertToUpper1(x):\n    return upper(x)\n\ndf.select(transform('skills',convertToUpper1).alias('skills')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f3ca308e-99a0-4353-ba2b-c63a68990e4c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+\n|           skills|\n+-----------------+\n|      [AZURE, ML]|\n|[PYTHON, PYSPARK]|\n+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["def fullname(x):\n    x=x+(x[0]+' '+x[1],)\n    return x\ndata=[('umair','memon'),('zee','shaik')]\ndf=spark.createDataFrame(data,['fn','ln'])\nrdd1=df.rdd.map(lambda x: fullname(x))\ndf1=rdd1.toDF(['fn','ln','Fullname'])\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f554749e-65c4-4559-9ffc-68ff83f8b583","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+-----------+\n|   fn|   ln|   Fullname|\n+-----+-----+-----------+\n|umair|memon|umair memon|\n|  zee|shaik|  zee shaik|\n+-----+-----+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[('umair','memon'),('pihu','patel')]\ndf=spark.createDataFrame(data,['fn','ln'])\nrdd1=df.rdd.map(lambda x: x+(x[0]+x[1],))\ndf1=rdd1.toDF(['fn','ln','fullname'])\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2b6abd7-5aaa-4e7f-8eda-62d600cd1b2b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+----------+\n|   fn|   ln|  fullname|\n+-----+-----+----------+\n|umair|memon|umairmemon|\n| pihu|patel| pihupatel|\n+-----+-----+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df1=spark.range(start=1,end=100)\n\ndf2=df1.sample(fraction=0.2,seed=123)\n\ndf3=df1.sample(fraction=0.1,seed=123)\n\ndf2.display()\ndf3.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a579625-1747-46ed-9176-76cbd1c20f0c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1],[5],[13],[22],[29],[34],[35],[36],[39],[42],[46],[48],[71],[82],[84],[87],[94],[99]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>1</td></tr><tr><td>5</td></tr><tr><td>13</td></tr><tr><td>22</td></tr><tr><td>29</td></tr><tr><td>34</td></tr><tr><td>35</td></tr><tr><td>36</td></tr><tr><td>39</td></tr><tr><td>42</td></tr><tr><td>46</td></tr><tr><td>48</td></tr><tr><td>71</td></tr><tr><td>82</td></tr><tr><td>84</td></tr><tr><td>87</td></tr><tr><td>94</td></tr><tr><td>99</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[35],[39],[42],[46],[71],[84],[87],[99]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th></tr></thead><tbody><tr><td>35</td></tr><tr><td>39</td></tr><tr><td>42</td></tr><tr><td>46</td></tr><tr><td>71</td></tr><tr><td>84</td></tr><tr><td>87</td></tr><tr><td>99</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.select('*', TotalPay(col('salary'),col('bonus'))).alias('totalpay').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5001fc1d-9487-4e19-99b8-3043a0848ae2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----+------+-----+-----------------------+\n| id| name|salary|bonus|<lambda>(salary, bonus)|\n+---+-----+------+-----+-----------------------+\n|  1|umair|  2000|  500|                   2500|\n|  2| pihu|  3000|  200|                   3200|\n|  3| annu|  2200|  100|                   2300|\n+---+-----+------+-----+-----------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 38 createOrReplaceTempView() function in pyspark \nadvantage of spark, you can work with SQL along with DataFrame that Means if you are confortable with sql, you can create temparray view on dataframe by using createOrReplacetempView() and use sql to select and manipulate data.\n\ntemp views are session scoped and can not be shared between the sessions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e32c3534-3f62-4243-b807-9aa40f4a019f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(dataRows[0])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f3e2b51-6160-4a50-8099-5bf606c92037","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Row(id=1, name='umair', gender='male', salary=100, dep=None)\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 43 FlatMap() transformation in pyspark\n\nFlatMap() is a transformation operation that flatten the RDD (array/map DataFrame Column) after applying the function on every element and return a new pyspark RDD.\nit's not available in dataframe. Explode() functions can be used in dataframe to flatten arrays."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8698aca-2e0b-4f43-a192-3ac2b777ac7a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=['umair memon','pihu patel']\nrdd=spark.sparkContext.parallelize(data)\n\nfor item in rdd.collect():\n    print(item)\n    \nrdd1 = rdd.flatMap(lambda x: x.split(' '))\nfor item in rdd1.collect():\n    print(item)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"528382d5-b09a-4ae3-9401-3e8293a8b384","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["umair memon\npihu patel\numair\nmemon\npihu\npatel\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 44 partitionBy() function in pyspark\nits used to partition large Dataset into samller file based on one or multiple columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8cda82d1-654b-4178-9943-029f4e639e40","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[(1,'umair','male','IT'),(2,'pihu','female','HR'),(3,'annu','female','IT')]\nschema=['id','name','gender','dep']\ndf=spark.createDataFrame(data,schema)\ndf.write.parquet(path='dbfs:/FileStore/using_partition',mode='overwrite',partitionBy='dep')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9bbfa2b6-df38-4178-a9d2-39b5286e332d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.write.parquet(path='dbfs:/FileStore/using_partition1',mode='overwrite',partitionBy=['dep','gender'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a57316a-b919-4f0e-a636-899f714436ea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# 45 from_json() function to convert json string into MapType\npre-requisite : MapType in pyspark\n\nit is used to convert json string into mapType or StructType in this we discuss about converting into mapType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d79630a4-5664-48dd-8695-47c44166a690","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[('umair','{\"hair\":\"black\",\"eye\":\"brown\"}')]\nschema=['name','props']\ndf=spark.createDataFrame(data,schema)\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18b14328-2d39-45e4-893a-51608063c7ae","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------------------------------+\n|name |props                         |\n+-----+------------------------------+\n|umair|{\"hair\":\"black\",\"eye\":\"brown\"}|\n+-----+------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fbe1f787-b5e3-4ef8-b219-ef975d7fe27c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- props: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import from_json \nfrom pyspark.sql.types import *\n\n# propsMap column with MapType generates from json string\n\n# propsMap column with MapType generates from json string\ndf1=df.withColumn('propsMap',from_json(df.props,MapType(StringType(),StringType())))\ndf1.show(truncate=False)\ndf1.printSchema()\n#  accessing 'eye' key from MapType column 'propMap'\n\ndf2=df1.withColumn('eye',df1.propsMap.eye).withColumn('hair',df1.propsMap.hair)\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d01666fa-2b67-4989-a29d-11a36a51074d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------------------------------+-----------------------------+\n|name |props                         |propsMap                     |\n+-----+------------------------------+-----------------------------+\n|umair|{\"hair\":\"black\",\"eye\":\"brown\"}|{hair -> black, eye -> brown}|\n+-----+------------------------------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- props: string (nullable = true)\n |-- propsMap: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+-----+------------------------------+-----------------------------+-----+-----+\n|name |props                         |propsMap                     |eye  |hair |\n+-----+------------------------------+-----------------------------+-----+-----+\n|umair|{\"hair\":\"black\",\"eye\":\"brown\"}|{hair -> black, eye -> brown}|brown|black|\n+-----+------------------------------+-----------------------------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# from_json() function to convert json string into StructType\n\npre-requisite : StructType() & StructField() in pyspark\n\nit is used to convert json string into MapType or StructType. in this we discuss about converting into MapType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d52bda68-2707-47e1-8563-f81eb293ef24","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[('umair',\"{'hair':'black','eye':'brown'}\")]\nschema=['name','props']\ndf=spark.createDataFrame(data,schema)\ndf.show(truncate=False)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07935b9b-73bb-4c24-80e4-2dc55419476b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------------------------------+\n|name |props                         |\n+-----+------------------------------+\n|umair|{'hair':'black','eye':'brown'}|\n+-----+------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- props: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import from_json\nfrom pyspark.sql.types import MapType, StructType, StructField"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23f06ec8-d5a6-4760-ac8c-a7b1f6d5633c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2=df1.withColumn('eye',df1.propsMap.eye).withColumn('hair',df1.propsMap.hair)\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f238a167-2bad-4354-918e-2b0e7d2aca4b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------------------------------+--------------+-----+-----+\n|name |props                         |propsMap      |eye  |hair |\n+-----+------------------------------+--------------+-----+-----+\n|umair|{'hair':'black','eye':'brown'}|{black, brown}|brown|black|\n+-----+------------------------------+--------------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StringType, StructType, StructField \nfrom pyspark.sql.functions import to_json"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6fe5bce4-ab2c-4ed5-910a-d0ef3eb9b40e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2=df.select(df.name,json_tuple(df.props,'hair','skin').alias('hair','skin'))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39fd2163-018a-47d7-989d-56c28ac11af1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+-----+\n| name| hair| skin|\n+-----+-----+-----+\n|umair|black|brown|\n| pihu|brown|white|\n+-----+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 49 get_json_object function in pyspark\n.from_json() .to_json() .json_tuple()\n\nit is used to extract the json string basrd on path from the json column."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12a9f557-18ab-4ad9-b9bb-d9b934c98594","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data=[('umair',\"{'hair':'black','eye':'brown','skin':'brown'}\"),('pihu',\"{'hair':'brown','eye':'black','skin':'white'}\")]\nschema=['name','props']\ndf=spark.createDataFrame(data,schema)\ndf.show(truncate=False)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e4bc5cd-569c-4f0f-82cc-b7b4a7d55825","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+---------------------------------------------+\n|name |props                                        |\n+-----+---------------------------------------------+\n|umair|{'hair':'black','eye':'brown','skin':'brown'}|\n|pihu |{'hair':'brown','eye':'black','skin':'white'}|\n+-----+---------------------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- props: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import json_tuple"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45b39a98-37ac-4355-8d5a-00afe4647d23","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# 47 to_json() function in pyspark\npre-requisite : StructType() & StructField in pyspark Maptype in pyspark\n\nto_json() in used to convert DataFrame column MaoType or StructType to json string"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"244d733c-77c2-44bb-be35-5d7e34605534","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["structschema = StructType([StructField('hair',StringType()),StructField('eye',StringType())])\ndf1=df.withColumn('propsMap',from_json(df.props,structschema))\ndf1.show(truncate=False)\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f25ec85-ba7d-48aa-be3f-cab093a8dc15","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------------------------------+--------------+\n|name |props                         |propsMap      |\n+-----+------------------------------+--------------+\n|umair|{'hair':'black','eye':'brown'}|{black, brown}|\n+-----+------------------------------+--------------+\n\nroot\n |-- name: string (nullable = true)\n |-- props: string (nullable = true)\n |-- propsMap: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[('umair',{'hair':'black','eye':'brown'})]\nschema=['name','props']\ndf=spark.createDataFrame(data,schema)\ndf.show(truncate=False)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e827ec1c-b250-4b24-8d7f-32080e030da6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----------------------------+\n|name |props                        |\n+-----+-----------------------------+\n|umair|{eye -> brown, hair -> black}|\n+-----+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- props: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[('umair',\"{'hair':'black','eye':'brown','skin':'brown'}\"),('pihu',\"{'hair':'brown','eye':'black','skin':'white'}\")]\nschema=['name','props']\ndf=spark.createDataFrame(data,schema)\ndf.show(truncate=False)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ee65f96-9ca9-4ffc-8b2a-451ed5a3650f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+---------------------------------------------+\n|name |props                                        |\n+-----+---------------------------------------------+\n|umair|{'hair':'black','eye':'brown','skin':'brown'}|\n|pihu |{'hair':'brown','eye':'black','skin':'white'}|\n+-----+---------------------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- props: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# here 'props_j' column will get generate as json string \ndf1=df.withColumn('props_j',to_json(df.props))\ndf1.show()\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6ddc0205-bcb1-49d9-b5dd-0c724dcbdacb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+--------------------+--------------------+\n| name|               props|             props_j|\n+-----+--------------------+--------------------+\n|umair|{eye -> brown, ha...|{\"eye\":\"brown\",\"h...|\n+-----+--------------------+--------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- props: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- props_j: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# 48 json_tuple() function in pyspark\npre-requisite: from_json() , to_json()\n\njson_tuple()function is used to query or extract elements from json string column and create as new columns."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bd1bb208-3ea0-404e-8099-ccb1efb9437f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import get_json_object\ndf1=df.select('name',get_json_object('props','$props.hair').alias('baal'))\ndf1.show(truncate=True)\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37ea2b60-fa92-4142-96d9-36cf23acb206","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+----+\n| name|baal|\n+-----+----+\n|umair|null|\n| pihu|null|\n+-----+----+\n\nroot\n |-- name: string (nullable = true)\n |-- baal: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["d\n# 50 date function in pyspark\n.current_date() .date_formate() .to_date()\n\nDateType default format is yyyy-MM-dd\n\ncurrent_date() get the current system date by default date will be return in yyyy-MM-dd format\nto_date() convert date string in datatype we need to specify format of date in the string in the function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"803a0ba4-1ef1-4692-a831-96ca97b8ac1b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import current_date, date_format, lit, to_date\ndf=spark.range(1)\n# gives current date in yyyy-MM-dd \ndf2=df.withColumn('todaysDate',current_date()).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48045136-e7b1-4610-b758-3c1d90656296","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|todaysDate|\n+---+----------+\n|  0|2023-03-12|\n+---+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# convert yyyy/MM/dd datatypes to specified format\ndf.withColumn('newformat',date_format(lit('2023-03-12'),('MM.dd.yyyy'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9de8799f-2c60-4277-993d-1d5a0d2617b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id| newformat|\n+---+----------+\n|  0|03.12.2023|\n+---+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# convert string values of date to DateType\ndf1=df.withColumn('newDateCol',to_date(lit('12.03.2023'),('dd.MM.yyyy'))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9edb671-b080-4002-90cf-4bdc291bd938","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|newDateCol|\n+---+----------+\n|  0|2023-03-12|\n+---+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# df1.printSchema()\n# df2.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e06856bb-3bd0-452f-a8bf-e19751a64dcd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# 51 date functions continuation\ndatediff(); Months_betwwen();add_months()\ndate_add(); year();month()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e4cd6f6-7853-489e-a6cb-39d665c50a3d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import datediff, months_between, add_months, date_add, year, month\ndf=spark.createDataFrame([('2015-04-08','2015-05-08')],['d1','d2'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1366c4db-04bf-4677-b9bb-0e8117daf917","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.withColumn('diff',datediff(df.d2,df.d1)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"118b0ab3-1b09-453b-9beb-31bdddf9b2c5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+----------+----+\n|        d1|        d2|diff|\n+----------+----------+----+\n|2015-04-08|2015-05-08|  30|\n+----------+----------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn('monthsBetween',months_between(df.d2,df.d1)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1453d4b8-9af1-4be0-946b-0e21ea7201b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+----------+-------------+\n|        d1|        d2|monthsBetween|\n+----------+----------+-------------+\n|2015-04-08|2015-05-08|          1.0|\n+----------+----------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn('addmonths',add_months())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"999879ff-570c-40aa-bc2c-ed98c652a1ad","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5f8ed60-54ff-4c59-8110-f2e5b3e6036d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49863520-2008-4b03-ad48-cc82a42079ab","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81684b8e-6e25-4e3a-a567-b969228a72ba","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e67858b-3bfb-4715-b9b1-ec1e91994c6a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a5a61dc-1201-40e2-9cdd-8401cc87b0c6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1ea5d1b-60ba-437b-b378-0d678ed1794e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5755a23b-9448-483c-bfcc-e0fadc233aa8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5cef239c-e3d3-4e3e-b179-32a406e8ee5e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c55b9456-9ddf-425f-8801-7548b6604481","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0b28130-3e34-48a7-abdd-b66cb18d1ea2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c10dfc0e-e637-42b5-bdf4-e777b312907f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"018addf8-6285-4e85-8db4-67b1c23c0b9f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5d3cb491-023c-44fe-88db-39845d4b3c47","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, explode\ndf = spark.createDataFrame([[[[('k1','v1', 'v2')]]]], ['d'])\ndf.display()\ndf2 = df.select(explode(col('d')).alias('d')).select(explode(col('d')).alias('d')).select(\"d.*\").drop(\"d\")\ndf2.printSchema()\ndf2.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a1086c2-aacc-4263-8b57-4959a19b4004","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[[[["k1","v1","v2"]]]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"d","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"_1\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_2\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_3\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"containsNull\":true},\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>d</th></tr></thead><tbody><tr><td>List(List(List(k1, v1, v2)))</td></tr></tbody></table></div>"]}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n |-- _3: string (nullable = true)\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["k1","v1","v2"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"_1","type":"\"string\"","metadata":"{}"},{"name":"_2","type":"\"string\"","metadata":"{}"},{"name":"_3","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_1</th><th>_2</th><th>_3</th></tr></thead><tbody><tr><td>k1</td><td>v1</td><td>v2</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15faa0cf-8e6c-439f-bb38-e9f406f6c629","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark wafastudies","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":4414599314557648,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":3584585115645377}},"nbformat":4,"nbformat_minor":0}
